from flask import Blueprint, request, jsonify, Response, stream_with_context
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
import os
import time
import json
import threading
from typing import Dict, List, Optional, Any
import anthropic
import openai
from openai import OpenAI
from config.config import AI_PROVIDERS, ANALYSIS_MODES, TOKEN_PRICING, RATE_LIMITS, MODEL_LIMITS, CLAUDE_API_KEY, REALTEK_API_KEY, REALTEK_BASE_URL
from enum import Enum

ai_analyzer_bp = Blueprint('ai_analyzer_bp', __name__)

# å…¨åŸŸè®Šæ•¸ç”¨æ–¼ç®¡ç†é€²è¡Œä¸­çš„åˆ†æ
active_analyses: Dict[str, 'AnalysisSession'] = {}

class AnalysisMode(Enum):
    SMART = "smart"
    QUICK = "quick" 
    DEEP = "deep"

class MessageRole(Enum):
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"

@dataclass
class Message:
    role: MessageRole
    content: str
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

@dataclass
class AnalysisContext:
    file_path: str
    file_content: str
    mode: AnalysisMode
    previous_messages: List[Message]
    metadata: Dict[str, Any]

class AIProvider(ABC):
    """AI Provider çš„æŠ½è±¡åŸºé¡"""
    
    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model
        self._stop_flag = threading.Event()
        
    @abstractmethod
    def analyze_sync(self, context: AnalysisContext) -> str:
        """åŒæ­¥åˆ†æ"""
        pass
    
    @abstractmethod
    def stream_analyze_sync(self, context: AnalysisContext):
        """åŒæ­¥æµå¼åˆ†æ"""
        pass
    
    def calculate_tokens(self, text: str) -> int:
        """è¨ˆç®— token æ•¸é‡"""
        return len(text) // 4  # ç°¡å–®ä¼°ç®—
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è¨ˆç®—æˆæœ¬"""
        return 0.0  # ç°¡åŒ–ç‰ˆæœ¬
    
    def stop(self):
        """åœæ­¢ç•¶å‰çš„åˆ†æ"""
        self._stop_flag.set()
    
    def reset_stop_flag(self):
        """é‡ç½®åœæ­¢æ¨™è¨˜"""
        self._stop_flag.clear()
    
    def should_stop(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢"""
        return self._stop_flag.is_set()

class RealtekProvider(AIProvider):
    """Realtek AI Provider - ä½¿ç”¨ OpenAI ç›¸å®¹ API"""
    
    def __init__(self, api_key: str, model: str, base_url: str = None):
        super().__init__(api_key, model)
        self.base_url = base_url or REALTEK_BASE_URL
        self.client = OpenAI(
            api_key=api_key,
            base_url=self.base_url
        )
        
        # å¾é…ç½®ç²å–æ¨¡å‹é™åˆ¶
        self.model_config = MODEL_LIMITS.get(model, {})
        self.max_tokens = self.model_config.get('max_tokens', 128000)
        self.max_output_tokens = self.model_config.get('max_output_tokens', 8000)
        self.chars_per_token = self.model_config.get('chars_per_token', 2.5)
    
    def calculate_tokens(self, text: str) -> int:
        """è¨ˆç®— token æ•¸é‡"""
        if not text:
            return 0
        return int(len(text) / self.chars_per_token)
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è¨ˆç®—æˆæœ¬"""
        pricing = TOKEN_PRICING.get('realtek', {}).get(self.model, {'input': 0, 'output': 0})
        input_cost = (input_tokens / 1000) * pricing.get('input', 0)
        output_cost = (output_tokens / 1000) * pricing.get('output', 0)
        return input_cost + output_cost
    
    def _truncate_content(self, content: str, mode: AnalysisMode) -> tuple[str, bool]:
        """æ ¹æ“š token é™åˆ¶æˆªå–å…§å®¹"""
        # æ ¹æ“šæ¨¡å‹èª¿æ•´é ç•™ tokens
        if self.model == 'chat-chattek-qwen':
            reserved_tokens = 5000  # 256K æ¨¡å‹ï¼Œé ç•™æ›´å¤šç©ºé–“
        else:  # chat-chattek-gpt (128K)
            reserved_tokens = 3000  # 128K æ¨¡å‹ï¼Œé ç•™è¼ƒå°‘ç©ºé–“
        
        if mode == AnalysisMode.QUICK:
            if self.model == 'chat-chattek-qwen':
                max_content_tokens = min(50000, self.max_tokens - reserved_tokens)  # 256K æ¨¡å‹å¯ä»¥æ›´å¤šå…§å®¹
            else:
                max_content_tokens = min(30000, self.max_tokens - reserved_tokens)  # 128K æ¨¡å‹è¼ƒå°‘å…§å®¹
        elif mode == AnalysisMode.DEEP:
            max_content_tokens = self.max_tokens - reserved_tokens
        else:  # SMART
            if self.model == 'chat-chattek-qwen':
                max_content_tokens = min(100000, self.max_tokens - reserved_tokens)  # 256K æ¨¡å‹
            else:
                max_content_tokens = min(60000, self.max_tokens - reserved_tokens)   # 128K æ¨¡å‹
        
        current_tokens = self.calculate_tokens(content)
        
        if current_tokens <= max_content_tokens:
            return content, False
        
        max_chars = int(max_content_tokens * self.chars_per_token)
        
        if mode == AnalysisMode.QUICK:
            head_chars = max_chars // 3
            tail_chars = max_chars // 3
            
            truncated = (
                content[:head_chars] + 
                f"\n\n... [çœç•¥ä¸­é–“å…§å®¹ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ] ...\n\n" +
                content[-tail_chars:]
            )
        else:
            truncated = content[:max_chars] + f"\n\n... [å…§å®¹å·²æˆªå–ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ]"
        
        return truncated, True
    
    def analyze_sync(self, context: AnalysisContext) -> str:
        """åŒæ­¥åˆ†æ"""
        try:
            messages = self._prepare_messages(context)
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=self._get_max_tokens(context.mode),
                temperature=self._get_temperature(context.mode),
                stream=False
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f"Realtek API éŒ¯èª¤: {str(e)}")
    
    def stream_analyze_sync(self, context: AnalysisContext):
        """åŒæ­¥æµå¼åˆ†æ"""
        self.reset_stop_flag()
        try:
            messages = self._prepare_messages(context)
            
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=self._get_max_tokens(context.mode),
                temperature=self._get_temperature(context.mode),
                stream=True
            )
            
            for chunk in stream:
                if self.should_stop():
                    break
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            yield f"\n\néŒ¯èª¤: {str(e)}"
    
    def _prepare_messages(self, context: AnalysisContext) -> List[Dict]:
        """æº–å‚™è¨Šæ¯æ ¼å¼"""
        messages = []
        
        # æˆªå–æª”æ¡ˆå…§å®¹
        truncated_content, was_truncated = self._truncate_content(
            context.file_content, 
            context.mode
        )
        
        # æ·»åŠ ç³»çµ±æç¤º
        system_prompt = self._get_system_prompt(context.mode)
        
        if was_truncated:
            system_prompt += "\n\næ³¨æ„ï¼šç”±æ–¼æª”æ¡ˆéå¤§ï¼Œåªæä¾›äº†éƒ¨åˆ†å…§å®¹é€²è¡Œåˆ†æã€‚è«‹åŸºæ–¼å¯è¦‹çš„å…§å®¹æä¾›åˆ†æã€‚"
        
        messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ æ­·å²è¨Šæ¯ï¼ˆé™åˆ¶æ•¸é‡ï¼‰
        recent_messages = context.previous_messages[-2:]  # Realtek æ¨¡å‹ä¿ç•™æ›´å°‘æ­·å²
        for msg in recent_messages:
            if msg.role == MessageRole.USER:
                content = msg.content[:800] if len(msg.content) > 800 else msg.content
                messages.append({"role": "user", "content": content})
            elif msg.role == MessageRole.ASSISTANT:
                content = msg.content[:1500] if len(msg.content) > 1500 else msg.content
                messages.append({"role": "assistant", "content": content})
        
        # æ·»åŠ ç•¶å‰è«‹æ±‚
        user_message = f"æª”æ¡ˆè·¯å¾‘: {context.file_path}\n\næª”æ¡ˆå…§å®¹:\n{truncated_content}"
        messages.append({"role": "user", "content": user_message})
        
        # å°‡æˆªå–ç‹€æ…‹ä¿å­˜åˆ°ä¸Šä¸‹æ–‡
        context.metadata['was_truncated'] = was_truncated
        context.metadata['original_size'] = len(context.file_content)
        context.metadata['truncated_size'] = len(truncated_content)
        context.metadata['actual_input_tokens'] = self.calculate_tokens(user_message)
        
        return messages
    
    def _get_system_prompt(self, mode: AnalysisMode) -> str:
        """æ ¹æ“šæ¨¡å¼ç²å–ç³»çµ±æç¤º"""
        prompts = {
            AnalysisMode.SMART: """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ Android ç³»çµ±æ—¥èªŒåˆ†æå°ˆå®¶ã€‚è«‹åˆ†ææä¾›çš„æ—¥èªŒå…§å®¹ï¼š

1. è‡ªå‹•è­˜åˆ¥æ—¥èªŒé¡å‹ï¼š
   - å¦‚æœåŒ…å« "Subject: Input dispatching timed out" æˆ–é¡ä¼¼å…§å®¹ï¼Œé€™æ˜¯ ANR (Application Not Responding) æ—¥èªŒ
   - å¦‚æœåŒ…å« "tombstone" æˆ–å´©æ½°å †ç–Šï¼Œé€™æ˜¯å´©æ½°æ—¥èªŒ
   - å…¶ä»–é¡å‹è«‹ç›¸æ‡‰è­˜åˆ¥

2. æ ¹æ“šæ—¥èªŒé¡å‹æä¾›åˆ†æï¼š
   - **å•é¡Œæ‘˜è¦**ï¼šç°¡æ½”æè¿°ç™¼ç”Ÿäº†ä»€éº¼å•é¡Œ
   - **æ ¹æœ¬åŸå› **ï¼šæ·±å…¥åˆ†æå°è‡´å•é¡Œçš„åŸå› 
   - **å½±éŸ¿ç¯„åœ**ï¼šå“ªäº›é€²ç¨‹/çµ„ä»¶å—åˆ°å½±éŸ¿
   - **è§£æ±ºæ–¹æ¡ˆ**ï¼šæä¾›å…·é«”çš„ä¿®å¾©å»ºè­°
   
3. è«‹ä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå’Œæ ¼å¼åŒ–è¼¸å‡ºï¼Œå›ç­”è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡""",
            
            AnalysisMode.QUICK: """ä½ æ˜¯ä¸€å€‹å¿«é€Ÿè¨ºæ–·åŠ©æ‰‹ã€‚è«‹å¿«é€Ÿåˆ†æé€™å€‹ Android æ—¥èªŒï¼š

å¿«é€Ÿè­˜åˆ¥ï¼š
- æ—¥èªŒé¡å‹ï¼ˆANR/å´©æ½°/å…¶ä»–ï¼‰
- ä¸»è¦å•é¡Œ
- ç«‹å³å¯åŸ·è¡Œçš„è§£æ±ºæ–¹æ¡ˆ

è«‹ä¿æŒç°¡æ½”ï¼Œåªæä¾›æœ€é—œéµçš„ä¿¡æ¯ï¼Œå›ç­”è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚""",
            
            AnalysisMode.DEEP: """ä½ æ˜¯ä¸€å€‹ç´°ç·»çš„ Android ç³»çµ±åˆ†æå°ˆå®¶ã€‚è«‹å°é€™å€‹æ—¥èªŒé€²è¡Œæ·±åº¦åˆ†æï¼š

1. **æ—¥èªŒè§£æ**
   - å®Œæ•´è§£è®€æ¯å€‹é—œéµä¿¡æ¯
   - åˆ†æè¨˜æ†¶é«”åœ°å€ã€æš«å­˜å™¨ç‹€æ…‹
   - è¿½è¹¤å®Œæ•´çš„èª¿ç”¨å †ç–Š

2. **å•é¡Œè¨ºæ–·**
   - è©³ç´°çš„æ ¹å› åˆ†æ
   - å¯èƒ½çš„è§¸ç™¼æ¢ä»¶
   - ç³»çµ±ç‹€æ…‹è©•ä¼°

3. **è§£æ±ºæ–¹æ¡ˆ**
   - ç¨‹å¼ç¢¼å±¤é¢çš„ä¿®å¾©å»ºè­°
   - ç³»çµ±é…ç½®å„ªåŒ–
   - é é˜²æªæ–½

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæä¾›è©³ç´°ä¸”å°ˆæ¥­çš„åˆ†æã€‚"""
        }
        return prompts.get(mode, prompts[AnalysisMode.SMART])
    
    def _get_max_tokens(self, mode: AnalysisMode) -> int:
        """æ ¹æ“šæ¨¡å¼ç²å–æœ€å¤§è¼¸å‡º token æ•¸"""
        base_max = self.max_output_tokens
        
        # æ ¹æ“šæ¨¡å‹èª¿æ•´è¼¸å‡º token é™åˆ¶
        if self.model == 'chat-chattek-qwen':  # 256K æ¨¡å‹ï¼Œå¯ä»¥æ›´å¤šè¼¸å‡º
            tokens = {
                AnalysisMode.QUICK: min(1000, base_max),
                AnalysisMode.SMART: min(2000, base_max),
                AnalysisMode.DEEP: min(4000, base_max)
            }
        else:  # chat-chattek-gpt (128K æ¨¡å‹)
            tokens = {
                AnalysisMode.QUICK: min(800, base_max),
                AnalysisMode.SMART: min(1500, base_max),
                AnalysisMode.DEEP: min(3000, base_max)
            }
        return tokens.get(mode, min(1500, base_max))
    
    def _get_temperature(self, mode: AnalysisMode) -> float:
        """æ ¹æ“šæ¨¡å¼ç²å–æº«åº¦åƒæ•¸"""
        temps = {
            AnalysisMode.QUICK: 0.3,
            AnalysisMode.SMART: 0.5,
            AnalysisMode.DEEP: 0.7
        }
        return temps.get(mode, 0.5)

class AnthropicProvider(AIProvider):
    """Anthropic Claude Provider"""
    
    def __init__(self, api_key: str, model: str):
        super().__init__(api_key, model)
        self.client = anthropic.Anthropic(api_key=api_key)
        
        # å¾é…ç½®ç²å–æ¨¡å‹é™åˆ¶
        self.model_config = MODEL_LIMITS.get(model, {})
        self.max_tokens = self.model_config.get('max_tokens', 200000)
        self.max_output_tokens = self.model_config.get('max_output_tokens', 4096)
        self.chars_per_token = self.model_config.get('chars_per_token', 4)
    
    def calculate_tokens(self, text: str) -> int:
        """è¨ˆç®— token æ•¸é‡"""
        if not text:
            return 0
        return len(text) // self.chars_per_token
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è¨ˆç®—æˆæœ¬"""
        pricing = TOKEN_PRICING.get('anthropic', {}).get(self.model, {'input': 0, 'output': 0})
        input_cost = (input_tokens / 1000) * pricing.get('input', 0)
        output_cost = (output_tokens / 1000) * pricing.get('output', 0)
        return input_cost + output_cost
    
    def _truncate_content(self, content: str, mode: AnalysisMode) -> tuple[str, bool]:
        """æ ¹æ“š token é™åˆ¶æˆªå–å…§å®¹"""
        reserved_tokens = 5000
        
        if mode == AnalysisMode.QUICK:
            max_content_tokens = min(50000, self.max_tokens - reserved_tokens)
        elif mode == AnalysisMode.DEEP:
            max_content_tokens = self.max_tokens - reserved_tokens
        else:
            max_content_tokens = min(100000, self.max_tokens - reserved_tokens)
        
        current_tokens = self.calculate_tokens(content)
        
        if current_tokens <= max_content_tokens:
            return content, False
        
        max_chars = max_content_tokens * self.chars_per_token
        
        if mode == AnalysisMode.QUICK:
            head_chars = max_chars // 3
            tail_chars = max_chars // 3
            
            truncated = (
                content[:head_chars] + 
                f"\n\n... [çœç•¥ä¸­é–“å…§å®¹ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ] ...\n\n" +
                content[-tail_chars:]
            )
        else:
            truncated = content[:max_chars] + f"\n\n... [å…§å®¹å·²æˆªå–ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ]"
        
        return truncated, True
    
    def analyze_sync(self, context: AnalysisContext) -> str:
        """åŒæ­¥åˆ†æ"""
        try:
            messages = self._prepare_messages(context)
            response = self.client.messages.create(
                model=self.model,
                messages=messages,
                max_tokens=self._get_max_tokens(context.mode),
                temperature=self._get_temperature(context.mode)
            )
            return response.content[0].text
        except Exception as e:
            raise Exception(f"Anthropic API éŒ¯èª¤: {str(e)}")
    
    def stream_analyze_sync(self, context: AnalysisContext):
        """åŒæ­¥æµå¼åˆ†æ"""
        self.reset_stop_flag()
        try:
            messages = self._prepare_messages(context)
            
            with self.client.messages.stream(
                model=self.model,
                messages=messages,
                max_tokens=self._get_max_tokens(context.mode),
                temperature=self._get_temperature(context.mode)
            ) as stream:
                for text in stream.text_stream:
                    if self.should_stop():
                        break
                    yield text
                    
        except Exception as e:
            yield f"\n\néŒ¯èª¤: {str(e)}"
    
    def _prepare_messages(self, context: AnalysisContext) -> List[Dict]:
        """æº–å‚™è¨Šæ¯æ ¼å¼"""
        messages = []
        
        truncated_content, was_truncated = self._truncate_content(
            context.file_content, 
            context.mode
        )
        
        system_prompt = self._get_system_prompt(context.mode)
        
        if was_truncated:
            system_prompt += "\n\næ³¨æ„ï¼šç”±æ–¼æª”æ¡ˆéå¤§ï¼Œåªæä¾›äº†éƒ¨åˆ†å…§å®¹é€²è¡Œåˆ†æã€‚è«‹åŸºæ–¼å¯è¦‹çš„å…§å®¹æä¾›åˆ†æã€‚"
        
        recent_messages = context.previous_messages[-3:]
        for msg in recent_messages:
            if msg.role == MessageRole.USER:
                content = msg.content[:1000] if len(msg.content) > 1000 else msg.content
                messages.append({"role": "user", "content": content})
            elif msg.role == MessageRole.ASSISTANT:
                content = msg.content[:2000] if len(msg.content) > 2000 else msg.content
                messages.append({"role": "assistant", "content": content})
        
        user_message = f"{system_prompt}\n\næª”æ¡ˆè·¯å¾‘: {context.file_path}\n\næª”æ¡ˆå…§å®¹:\n{truncated_content}"
        messages.append({"role": "user", "content": user_message})
        
        context.metadata['was_truncated'] = was_truncated
        context.metadata['original_size'] = len(context.file_content)
        context.metadata['truncated_size'] = len(truncated_content)
        context.metadata['actual_input_tokens'] = self.calculate_tokens(user_message)
        
        return messages
    
    def _get_system_prompt(self, mode: AnalysisMode) -> str:
        """æ ¹æ“šæ¨¡å¼ç²å–ç³»çµ±æç¤º"""
        prompts = {
            AnalysisMode.SMART: """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ Android ç³»çµ±æ—¥èªŒåˆ†æå°ˆå®¶ã€‚è«‹åˆ†ææä¾›çš„æ—¥èªŒå…§å®¹ï¼š

1. è‡ªå‹•è­˜åˆ¥æ—¥èªŒé¡å‹ï¼š
   - å¦‚æœåŒ…å« "Subject: Input dispatching timed out" æˆ–é¡ä¼¼å…§å®¹ï¼Œé€™æ˜¯ ANR (Application Not Responding) æ—¥èªŒ
   - å¦‚æœåŒ…å« "tombstone" æˆ–å´©æ½°å †ç–Šï¼Œé€™æ˜¯å´©æ½°æ—¥èªŒ
   - å…¶ä»–é¡å‹è«‹ç›¸æ‡‰è­˜åˆ¥

2. æ ¹æ“šæ—¥èªŒé¡å‹æä¾›åˆ†æï¼š
   - **å•é¡Œæ‘˜è¦**ï¼šç°¡æ½”æè¿°ç™¼ç”Ÿäº†ä»€éº¼å•é¡Œ
   - **æ ¹æœ¬åŸå› **ï¼šæ·±å…¥åˆ†æå°è‡´å•é¡Œçš„åŸå› 
   - **å½±éŸ¿ç¯„åœ**ï¼šå“ªäº›é€²ç¨‹/çµ„ä»¶å—åˆ°å½±éŸ¿
   - **è§£æ±ºæ–¹æ¡ˆ**ï¼šæä¾›å…·é«”çš„ä¿®å¾©å»ºè­°
   
3. è«‹ä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå’Œæ ¼å¼åŒ–è¼¸å‡º""",
            
            AnalysisMode.QUICK: """ä½ æ˜¯ä¸€å€‹å¿«é€Ÿè¨ºæ–·åŠ©æ‰‹ã€‚åœ¨ 30 ç§’å…§åˆ†æé€™å€‹ Android æ—¥èªŒï¼š

å¿«é€Ÿè­˜åˆ¥ï¼š
- æ—¥èªŒé¡å‹ï¼ˆANR/å´©æ½°/å…¶ä»–ï¼‰
- ä¸»è¦å•é¡Œ
- ç«‹å³å¯åŸ·è¡Œçš„è§£æ±ºæ–¹æ¡ˆ

è«‹ä¿æŒç°¡æ½”ï¼Œåªæä¾›æœ€é—œéµçš„ä¿¡æ¯ã€‚""",
            
            AnalysisMode.DEEP: """ä½ æ˜¯ä¸€å€‹ç´°ç·»çš„ Android ç³»çµ±åˆ†æå°ˆå®¶ã€‚è«‹å°é€™å€‹æ—¥èªŒé€²è¡Œæ·±åº¦åˆ†æï¼š

1. **æ—¥èªŒè§£æ**
   - å®Œæ•´è§£è®€æ¯å€‹é—œéµä¿¡æ¯
   - åˆ†æè¨˜æ†¶é«”åœ°å€ã€æš«å­˜å™¨ç‹€æ…‹
   - è¿½è¹¤å®Œæ•´çš„èª¿ç”¨å †ç–Š

2. **å•é¡Œè¨ºæ–·**
   - è©³ç´°çš„æ ¹å› åˆ†æ
   - å¯èƒ½çš„è§¸ç™¼æ¢ä»¶
   - ç³»çµ±ç‹€æ…‹è©•ä¼°

3. **è§£æ±ºæ–¹æ¡ˆ**
   - ç¨‹å¼ç¢¼å±¤é¢çš„ä¿®å¾©å»ºè­°
   - ç³»çµ±é…ç½®å„ªåŒ–
   - é é˜²æªæ–½

4. **ç›¸é—œçŸ¥è­˜**
   - ç›¸é—œçš„ Android ç³»çµ±çŸ¥è­˜
   - é¡ä¼¼å•é¡Œçš„è™•ç†ç¶“é©—"""
        }
        return prompts.get(mode, prompts[AnalysisMode.SMART])
    
    def _get_max_tokens(self, mode: AnalysisMode) -> int:
        """æ ¹æ“šæ¨¡å¼ç²å–æœ€å¤§è¼¸å‡º token æ•¸"""
        base_max = self.max_output_tokens
        
        tokens = {
            AnalysisMode.QUICK: min(1000, base_max),
            AnalysisMode.SMART: min(2000, base_max),
            AnalysisMode.DEEP: min(4000, base_max)
        }
        return tokens.get(mode, min(2000, base_max))
    
    def _get_temperature(self, mode: AnalysisMode) -> float:
        """æ ¹æ“šæ¨¡å¼ç²å–æº«åº¦åƒæ•¸"""
        temps = {
            AnalysisMode.QUICK: 0.3,
            AnalysisMode.SMART: 0.5,
            AnalysisMode.DEEP: 0.7
        }
        return temps.get(mode, 0.5)

class ProviderFactory:
    """AI Provider å·¥å» é¡"""
    
    @staticmethod
    def create_provider(provider_name: str, model: str) -> AIProvider:
        """å‰µå»º AI Provider å¯¦ä¾‹"""
        if provider_name not in AI_PROVIDERS:
            raise ValueError(f"ä¸æ”¯æ´çš„ Provider: {provider_name}")
        
        config = AI_PROVIDERS[provider_name]
        api_key = config['api_key']
        
        if not api_key:
            raise ValueError(f"{provider_name} API Key æœªè¨­ç½®")
        
        if provider_name == 'anthropic':
            return AnthropicProvider(api_key, model)
        elif provider_name == 'realtek':
            base_url = config.get('base_url', REALTEK_BASE_URL)
            return RealtekProvider(api_key, model, base_url)
        elif provider_name == 'openai':
            # å¦‚æœæ‚¨éœ€è¦ OpenAI æ”¯æŒï¼Œå¯ä»¥åœ¨é€™è£¡å¯¦ç¾
            raise ValueError("OpenAI Provider å°šæœªå¯¦ç¾")
        else:
            raise ValueError(f"æœªå¯¦ä½œçš„ Provider: {provider_name}")

class AnalysisSession:
    """åˆ†ææœƒè©±ç®¡ç†"""
    
    def __init__(self, session_id: str, provider: AIProvider):
        self.session_id = session_id
        self.provider = provider
        self.messages: List[Message] = []
        self.created_at = datetime.now()
        self.last_activity = datetime.now()
        self.is_active = True
        self.total_cost = 0.0
        self.total_tokens = {'input': 0, 'output': 0}
        
    def add_message(self, role: MessageRole, content: str):
        """æ·»åŠ è¨Šæ¯åˆ°æœƒè©±"""
        self.messages.append(Message(role, content))
        self.last_activity = datetime.now()
        
    def stop(self):
        """åœæ­¢ç•¶å‰åˆ†æ"""
        self.provider.stop()
        self.is_active = False
        
    def update_usage(self, input_tokens: int, output_tokens: int):
        """æ›´æ–°ä½¿ç”¨é‡çµ±è¨ˆ"""
        self.total_tokens['input'] += input_tokens
        self.total_tokens['output'] += output_tokens
        self.total_cost += self.provider.get_cost(input_tokens, output_tokens)

# API è·¯ç”±
@ai_analyzer_bp.route('/api/ai/analyze', methods=['POST'])
def analyze_file():
    """åŸ·è¡Œ AI åˆ†æ"""
    try:
        data = request.json
        session_id = data.get('session_id', str(time.time()))
        provider_name = data.get('provider', 'realtek')  # é è¨­ä½¿ç”¨ Realtek
        model = data.get('model', AI_PROVIDERS[provider_name]['default_model'])
        mode = AnalysisMode(data.get('mode', 'smart'))
        file_path = data.get('file_path', '')
        file_name = data.get('file_name', '')
        file_content = data.get('content', '')
        stream = data.get('stream', True)
        context_messages = data.get('context', [])
        
        # å‰µå»ºæˆ–ç²å–æœƒè©±
        if session_id in active_analyses:
            session = active_analyses[session_id]
        else:
            provider = ProviderFactory.create_provider(provider_name, model)
            session = AnalysisSession(session_id, provider)
            active_analyses[session_id] = session
        
        # æ·»åŠ ä¸Šä¸‹æ–‡è¨Šæ¯åˆ°æœƒè©±
        for msg in context_messages:
            if 'role' in msg and 'content' in msg:
                try:
                    role = MessageRole(msg['role']) if msg['role'] in ['user', 'assistant', 'system'] else MessageRole.USER
                    session.add_message(role, msg['content'])
                except ValueError:
                    session.add_message(MessageRole.USER, msg['content'])
        
        # æº–å‚™åˆ†æä¸Šä¸‹æ–‡
        context = AnalysisContext(
            file_path=file_path,
            file_content=file_content,
            mode=mode,
            previous_messages=session.messages,
            metadata={
                'session_id': session_id,
                'file_name': file_name
            }
        )
        
        if stream:
            def generate():
                """ç”Ÿæˆ SSE æµ"""
                try:
                    # ç™¼é€é–‹å§‹äº‹ä»¶
                    yield f"data: {json.dumps({'type': 'start', 'mode': mode.value})}\n\n"
                    
                    # æª¢æŸ¥æª”æ¡ˆå¤§å°å’Œæ¨¡å‹é¡å‹
                    file_size = len(file_content)
                    estimated_tokens = session.provider.calculate_tokens(file_content)
                    
                    # æ ¹æ“šä¸åŒ Provider é¡¯ç¤ºä¸åŒè³‡è¨Š
                    if provider_name == 'realtek':
                        file_info = f"æ­£åœ¨ä½¿ç”¨ Realtek å…§éƒ¨æ¨¡å‹åˆ†æ: {file_name or file_path}"
                    else:
                        file_info = f"æ­£åœ¨åˆ†ææª”æ¡ˆ: {file_name or file_path}"
                    
                    # è­˜åˆ¥æª”æ¡ˆé¡å‹
                    if 'tombstone' in file_path.lower() or 'tombstone' in file_content[:1000].lower():
                        file_info += " (Tombstone å´©æ½°æ—¥èªŒ)"
                    elif 'anr' in file_path.lower() or 'input dispatching timed out' in file_content[:5000].lower():
                        file_info += " (ANR æ—¥èªŒ)"
                    elif 'fatal' in file_content[:1000].lower() or 'crash' in file_content[:1000].lower():
                        file_info += " (å´©æ½°æ—¥èªŒ)"
                    
                    yield f"data: {json.dumps({'type': 'info', 'message': file_info})}\n\n"
                    
                    # é¡¯ç¤ºæª”æ¡ˆå¤§å°è³‡è¨Š
                    size_info = f"æª”æ¡ˆå¤§å°: {file_size:,} å­—å…ƒ (ç´„ {estimated_tokens:,} tokens)"
                    yield f"data: {json.dumps({'type': 'info', 'message': size_info})}\n\n"
                    
                    # æ ¹æ“šä¸åŒæ¨¡å‹é¡¯ç¤ºä¸åŒçš„ token é™åˆ¶è­¦å‘Š
                    model_config = MODEL_LIMITS.get(model, {})
                    max_tokens = model_config.get('max_tokens', 200000)
                    
                    if estimated_tokens > max_tokens * 0.8:
                        if provider_name == 'realtek':
                            warning_msg = f"âš ï¸ æª”æ¡ˆè¼ƒå¤§ï¼ˆç´„ {estimated_tokens:,} tokensï¼‰ï¼ŒRealtek æ¨¡å‹å°‡è‡ªå‹•æˆªå–é©ç•¶å…§å®¹"
                        else:
                            warning_msg = f"âš ï¸ æª”æ¡ˆè¼ƒå¤§ï¼ˆç´„ {estimated_tokens:,} tokensï¼‰ï¼Œå°‡è‡ªå‹•æˆªå–é©ç•¶å…§å®¹é€²è¡Œåˆ†æ"
                        yield f"data: {json.dumps({'type': 'warning', 'message': warning_msg})}\n\n"
                    
                    # æ ¹æ“šæ¨¡å¼é¡¯ç¤ºé æœŸæ™‚é–“
                    if mode == AnalysisMode.QUICK:
                        if provider_name == 'realtek':
                            yield f"data: {json.dumps({'type': 'info', 'message': 'âš¡ Realtek å¿«é€Ÿåˆ†ææ¨¡å¼ï¼šé è¨ˆ 20 ç§’å…§å®Œæˆ'})}\n\n"
                        else:
                            yield f"data: {json.dumps({'type': 'info', 'message': 'âš¡ å¿«é€Ÿåˆ†ææ¨¡å¼ï¼šé è¨ˆ 30 ç§’å…§å®Œæˆ'})}\n\n"
                    elif mode == AnalysisMode.DEEP:
                        yield f"data: {json.dumps({'type': 'info', 'message': 'ğŸ” æ·±åº¦åˆ†ææ¨¡å¼ï¼šé è¨ˆ 2-3 åˆ†é˜å®Œæˆ'})}\n\n"
                    else:
                        yield f"data: {json.dumps({'type': 'info', 'message': 'ğŸ§  æ™ºèƒ½åˆ†ææ¨¡å¼ï¼šè‡ªå‹•å¹³è¡¡é€Ÿåº¦èˆ‡æ·±åº¦'})}\n\n"
                    
                    # ä¼°ç®—è¼¸å…¥ tokens
                    yield f"data: {json.dumps({'type': 'tokens', 'input': estimated_tokens})}\n\n"
                    
                    # æµå¼è¼¸å‡º
                    output_content = ""
                    chunk_count = 0
                    last_update_time = time.time()
                    
                    try:
                        for chunk in session.provider.stream_analyze_sync(context):
                            # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                            if not session.is_active:
                                yield f"data: {json.dumps({'type': 'stopped', 'message': 'åˆ†æå·²è¢«ä½¿ç”¨è€…ä¸­æ–·'})}\n\n"
                                break
                            
                            output_content += chunk
                            chunk_count += 1
                            
                            # ç™¼é€å…§å®¹
                            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
                            
                            # å®šæœŸæ›´æ–° token è¨ˆæ•¸
                            current_time = time.time()
                            if current_time - last_update_time > 1.0:
                                output_tokens = session.provider.calculate_tokens(output_content)
                                yield f"data: {json.dumps({'type': 'tokens', 'output': output_tokens})}\n\n"
                                last_update_time = current_time
                    
                    except Exception as stream_error:
                        # è™•ç†æµå¼è¼¸å‡ºä¸­çš„éŒ¯èª¤
                        error_msg = str(stream_error)
                        
                        # é‡å°ä¸åŒ Provider æä¾›ä¸åŒçš„éŒ¯èª¤è™•ç†
                        if provider_name == 'realtek':
                            if "rate limit" in error_msg.lower():
                                error_msg = "Realtek API è«‹æ±‚é »ç¹ï¼Œè«‹ç¨å¾Œå†è©¦"
                            elif "prompt is too long" in error_msg.lower():
                                error_msg = "æª”æ¡ˆå…§å®¹éå¤§ï¼Œè«‹å˜—è©¦ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼"
                        else:
                            if "rate limit" in error_msg.lower():
                                error_msg = "è¶…é API é€Ÿç‡é™åˆ¶ï¼Œè«‹ç¨å¾Œå†è©¦ï¼ˆå»ºè­°ç­‰å¾… 30 ç§’ï¼‰"
                            elif "prompt is too long" in error_msg.lower():
                                error_msg = "æª”æ¡ˆå…§å®¹éå¤§ï¼Œè«‹å˜—è©¦ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼æˆ–åˆ†æ®µåˆ†æ"
                        
                        yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
                        return
                    
                    # å¦‚æœå…§å®¹è¢«æˆªå–ï¼Œåœ¨å®Œæˆæ™‚é€šçŸ¥
                    if context.metadata.get('was_truncated', False):
                        original_size = context.metadata.get('original_size', 0)
                        truncated_size = context.metadata.get('truncated_size', 0)
                        truncate_percentage = (truncated_size / original_size * 100) if original_size > 0 else 0
                        
                        truncate_info = {
                            'type': 'info',
                            'message': f"âœ‚ï¸ åˆ†æå®Œæˆã€‚ç”±æ–¼æª”æ¡ˆéå¤§ï¼Œå¯¦éš›åˆ†æäº† {truncated_size:,} / {original_size:,} å­—å…ƒ ({truncate_percentage:.1f}%)"
                        }
                        yield f"data: {json.dumps(truncate_info)}\n\n"
                    
                    # å®Œæˆåˆ†æ
                    if session.is_active and output_content:
                        # æ·»åŠ è¨Šæ¯åˆ°æœƒè©±æ­·å²
                        session.add_message(MessageRole.USER, f"åˆ†æ {mode.value} æ¨¡å¼: {file_name or file_path}")
                        session.add_message(MessageRole.ASSISTANT, output_content)
                        
                        # è¨ˆç®—æœ€çµ‚çš„ token ä½¿ç”¨é‡
                        final_output_tokens = session.provider.calculate_tokens(output_content)
                        actual_input_tokens = context.metadata.get('actual_input_tokens', estimated_tokens)
                        session.update_usage(actual_input_tokens, final_output_tokens)
                        
                        # ç™¼é€å®Œæˆäº‹ä»¶
                        yield f"data: {json.dumps({
                            'type': 'complete',
                            'usage': {
                                'input': session.total_tokens['input'],
                                'output': session.total_tokens['output'],
                                'total': session.total_tokens['input'] + session.total_tokens['output']
                            },
                            'cost': session.total_cost,
                            'was_truncated': context.metadata.get('was_truncated', False),
                            'duration': (datetime.now() - session.created_at).total_seconds(),
                            'provider': provider_name,
                            'model': model
                        })}\n\n"
                    
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    
                    error_msg = str(e)
                    # é‡å°ä¸åŒ Provider æä¾›æ›´å‹å¥½çš„éŒ¯èª¤è¨Šæ¯
                    if provider_name == 'realtek':
                        if "Connection" in error_msg or "timeout" in error_msg.lower():
                            error_msg = "Realtek å…§éƒ¨ API é€£ç·šè¶…æ™‚ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥"
                        elif "api key" in error_msg.lower():
                            error_msg = "Realtek API Key ç„¡æ•ˆæˆ–éæœŸï¼Œè«‹è¯ç¹«ç®¡ç†å“¡"
                    else:
                        if "prompt is too long" in error_msg:
                            error_msg = "æª”æ¡ˆå…§å®¹è¶…éæ¨¡å‹é™åˆ¶ï¼Œè«‹ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼æˆ–æ¸›å°‘æª”æ¡ˆå¤§å°"
                        elif "rate limit" in error_msg.lower():
                            error_msg = "API è«‹æ±‚éæ–¼é »ç¹ï¼Œè«‹ç­‰å¾… 30 ç§’å¾Œå†è©¦"
                        elif "api key" in error_msg.lower():
                            error_msg = "API Key ç„¡æ•ˆæˆ–æœªè¨­ç½®ï¼Œè«‹æª¢æŸ¥é…ç½®"
                    
                    yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
            
            return Response(
                stream_with_context(generate()),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no',
                    'Connection': 'keep-alive',
                    'Content-Type': 'text/event-stream'
                }
            )
        else:
            # éæµå¼å›æ‡‰
            try:
                result = session.provider.analyze_sync(context)
                
                was_truncated = context.metadata.get('was_truncated', False)
                
                session.add_message(MessageRole.USER, f"åˆ†ææª”æ¡ˆ: {file_name or file_path}")
                session.add_message(MessageRole.ASSISTANT, result)
                
                input_tokens = session.provider.calculate_tokens(file_content)
                output_tokens = session.provider.calculate_tokens(result)
                session.update_usage(input_tokens, output_tokens)
                
                response_data = {
                    'success': True,
                    'result': result,
                    'session_id': session_id,
                    'usage': {
                        'input': session.total_tokens['input'],
                        'output': session.total_tokens['output'],
                        'total': session.total_tokens['input'] + session.total_tokens['output']
                    },
                    'cost': session.total_cost,
                    'was_truncated': was_truncated,
                    'provider': provider_name,
                    'model': model
                }
                
                if was_truncated:
                    response_data['truncation_info'] = {
                        'original_size': context.metadata.get('original_size', 0),
                        'truncated_size': context.metadata.get('truncated_size', 0)
                    }
                
                return jsonify(response_data)
                
            except Exception as e:
                import traceback
                traceback.print_exc()
                
                error_msg = str(e)
                if provider_name == 'realtek':
                    if "Connection" in error_msg:
                        error_msg = "Realtek å…§éƒ¨ API é€£ç·šå¤±æ•—"
                else:
                    if "prompt is too long" in error_msg:
                        error_msg = "æª”æ¡ˆå…§å®¹è¶…éæ¨¡å‹é™åˆ¶"
                    elif "rate limit" in error_msg.lower():
                        error_msg = "API è«‹æ±‚é™åˆ¶"
                
                return jsonify({
                    'success': False,
                    'error': error_msg,
                    'details': str(e),
                    'provider': provider_name
                }), 500
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'error': 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤',
            'details': str(e)
        }), 500

@ai_analyzer_bp.route('/api/ai/stop/<session_id>', methods=['POST'])
def stop_analysis(session_id):
    """åœæ­¢åˆ†æ"""
    if session_id in active_analyses:
        session = active_analyses[session_id]
        session.stop()
        return jsonify({'success': True, 'message': 'åˆ†æå·²åœæ­¢'})
    return jsonify({'success': False, 'error': 'æœƒè©±ä¸å­˜åœ¨'}), 404

@ai_analyzer_bp.route('/api/ai/models/<provider>', methods=['GET'])
def get_models(provider):
    """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    if provider not in AI_PROVIDERS:
        return jsonify({'error': 'ä¸æ”¯æ´çš„ Provider'}), 400
        
    models = AI_PROVIDERS[provider]['models']
    return jsonify({
        'models': [
            {
                'id': model_id,
                'name': info['name'],
                'description': info['description'],
                'max_tokens': info['max_tokens'],
                'pricing': TOKEN_PRICING.get(provider, {}).get(model_id, {}),
                'provider': provider
            }
            for model_id, info in models.items()
        ]
    })