from flask import Blueprint, request, jsonify, Response, stream_with_context
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
import os
import time
import json
import threading
from typing import Dict, List, Optional, Any
import anthropic
import openai
from openai import OpenAI
from config.config import AI_PROVIDERS, ANALYSIS_MODES, TOKEN_PRICING, RATE_LIMITS, MODEL_LIMITS, CLAUDE_API_KEY, REALTEK_API_KEY, REALTEK_BASE_URL
from enum import Enum
import requests

ai_analyzer_bp = Blueprint('ai_analyzer_bp', __name__)

# å…¨åŸŸè®Šæ•¸ç”¨æ–¼ç®¡ç†é€²è¡Œä¸­çš„åˆ†æ
active_analyses: Dict[str, 'AnalysisSession'] = {}

class AnalysisMode(Enum):
    SMART = "smart"
    QUICK = "quick" 
    DEEP = "deep"

class MessageRole(Enum):
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"

@dataclass
class Message:
    role: MessageRole
    content: str
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

@dataclass
class AnalysisContext:
    file_path: str
    file_content: str
    mode: AnalysisMode
    previous_messages: List[Message]
    metadata: Dict[str, Any]

class AIProvider(ABC):
    """AI Provider çš„æŠ½è±¡åŸºé¡"""
    
    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model
        self._stop_flag = threading.Event()
        
    @abstractmethod
    def analyze_sync(self, context: AnalysisContext) -> str:
        """åŒæ­¥åˆ†æ"""
        pass
    
    @abstractmethod
    def stream_analyze_sync(self, context: AnalysisContext):
        """åŒæ­¥æµå¼åˆ†æ"""
        pass
    
    def calculate_tokens(self, text: str) -> int:
        """è¨ˆç®— token æ•¸é‡"""
        return len(text) // 4  # ç°¡å–®ä¼°ç®—
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è¨ˆç®—æˆæœ¬"""
        return 0.0  # ç°¡åŒ–ç‰ˆæœ¬
    
    def stop(self):
        """åœæ­¢ç•¶å‰çš„åˆ†æ"""
        self._stop_flag.set()
    
    def reset_stop_flag(self):
        """é‡ç½®åœæ­¢æ¨™è¨˜"""
        self._stop_flag.clear()
    
    def should_stop(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢"""
        return self._stop_flag.is_set()

class RealtekProvider(AIProvider):
    """Realtek AI Provider - ä½¿ç”¨åŸç”Ÿ HTTP è«‹æ±‚é¿å… OpenAI Client å•é¡Œ"""
    
    def __init__(self, api_key: str, model: str, base_url: str = None):
        super().__init__(api_key, model)
        self.base_url = base_url or REALTEK_BASE_URL
        self.api_key = api_key
        
        # ä¸ä½¿ç”¨ OpenAI clientï¼Œæ”¹ç”¨ç›´æ¥çš„ HTTP è«‹æ±‚
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        
        # å¾é…ç½®ç²å–æ¨¡å‹é™åˆ¶
        self.model_config = MODEL_LIMITS.get(model, {})
        self.max_tokens = self.model_config.get('max_tokens', 128000)
        self.max_output_tokens = self.model_config.get('max_output_tokens', 8000)
        self.chars_per_token = self.model_config.get('chars_per_token', 2.5)
    
    def calculate_tokens(self, text: str) -> int:
        """è¨ˆç®— token æ•¸é‡"""
        if not text:
            return 0
        return int(len(text) / self.chars_per_token)
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è¨ˆç®—æˆæœ¬"""
        pricing = TOKEN_PRICING.get('realtek', {}).get(self.model, {'input': 0, 'output': 0})
        input_cost = (input_tokens / 1000) * pricing.get('input', 0)
        output_cost = (output_tokens / 1000) * pricing.get('output', 0)
        return input_cost + output_cost
    
    def _make_request(self, messages: list, max_tokens: int, temperature: float, stream: bool = False):
        """ç™¼é€ HTTP è«‹æ±‚åˆ° Realtek API"""
        url = f"{self.base_url}/chat/completions"
        
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "stream": stream
        }
        
        try:
            response = requests.post(
                url, 
                headers=self.headers, 
                json=payload, 
                stream=stream,
                timeout=300  # 5åˆ†é˜è¶…æ™‚
            )
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            raise Exception(f"Realtek API è«‹æ±‚å¤±æ•—: {str(e)}")
    
    def _truncate_content(self, content: str, mode: AnalysisMode) -> tuple[str, bool]:
        """æ ¹æ“š token é™åˆ¶æˆªå–å…§å®¹"""
        # æ ¹æ“šæ¨¡å‹èª¿æ•´é ç•™ tokens
        if self.model == 'chat-chattek-qwen':
            reserved_tokens = 5000  # 256K æ¨¡å‹ï¼Œé ç•™æ›´å¤šç©ºé–“
        else:  # chat-chattek-gpt (128K)
            reserved_tokens = 3000  # 128K æ¨¡å‹ï¼Œé ç•™è¼ƒå°‘ç©ºé–“
        
        if mode == AnalysisMode.QUICK:
            if self.model == 'chat-chattek-qwen':
                max_content_tokens = min(50000, self.max_tokens - reserved_tokens)  # 256K æ¨¡å‹å¯ä»¥æ›´å¤šå…§å®¹
            else:
                max_content_tokens = min(30000, self.max_tokens - reserved_tokens)  # 128K æ¨¡å‹è¼ƒå°‘å…§å®¹
        elif mode == AnalysisMode.DEEP:
            max_content_tokens = self.max_tokens - reserved_tokens
        else:  # SMART
            if self.model == 'chat-chattek-qwen':
                max_content_tokens = min(100000, self.max_tokens - reserved_tokens)  # 256K æ¨¡å‹
            else:
                max_content_tokens = min(60000, self.max_tokens - reserved_tokens)   # 128K æ¨¡å‹
        
        current_tokens = self.calculate_tokens(content)
        
        if current_tokens <= max_content_tokens:
            return content, False
        
        max_chars = int(max_content_tokens * self.chars_per_token)
        
        if mode == AnalysisMode.QUICK:
            head_chars = max_chars // 3
            tail_chars = max_chars // 3
            
            truncated = (
                content[:head_chars] + 
                f"\n\n... [çœç•¥ä¸­é–“å…§å®¹ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ] ...\n\n" +
                content[-tail_chars:]
            )
        else:
            truncated = content[:max_chars] + f"\n\n... [å…§å®¹å·²æˆªå–ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ]"
        
        return truncated, True
    
    def analyze_sync(self, context: AnalysisContext) -> str:
        """åŒæ­¥åˆ†æ"""
        try:
            messages = self._prepare_messages(context)
            max_tokens = self._get_max_tokens(context.mode)
            temperature = self._get_temperature(context.mode)
            
            response = self._make_request(messages, max_tokens, temperature, stream=False)
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                raise Exception("API å›æ‡‰æ ¼å¼ç•°å¸¸")
                
        except Exception as e:
            raise Exception(f"Realtek API éŒ¯èª¤: {str(e)}")
    
    def stream_analyze_sync(self, context: AnalysisContext):
        """åŒæ­¥æµå¼åˆ†æ"""
        self.reset_stop_flag()
        try:
            messages = self._prepare_messages(context)
            max_tokens = self._get_max_tokens(context.mode)
            temperature = self._get_temperature(context.mode)
            
            response = self._make_request(messages, max_tokens, temperature, stream=True)
            
            for line in response.iter_lines():
                if self.should_stop():
                    break
                    
                if line:
                    line_text = line.decode('utf-8')
                    if line_text.startswith('data: '):
                        data_text = line_text[6:]  # ç§»é™¤ 'data: ' å‰ç¶´
                        
                        if data_text.strip() == '[DONE]':
                            break
                            
                        try:
                            data = json.loads(data_text)
                            if 'choices' in data and len(data['choices']) > 0:
                                choice = data['choices'][0]
                                if 'delta' in choice and 'content' in choice['delta']:
                                    content = choice['delta']['content']
                                    if content:
                                        yield content
                        except json.JSONDecodeError:
                            continue  # è·³éç„¡æ³•è§£æçš„è¡Œ
                            
        except Exception as e:
            yield f"\n\néŒ¯èª¤: {str(e)}"
    
    def _prepare_messages(self, context: AnalysisContext) -> List[Dict]:
        """æº–å‚™è¨Šæ¯æ ¼å¼"""
        messages = []
        
        # æˆªå–æª”æ¡ˆå…§å®¹
        truncated_content, was_truncated = self._truncate_content(
            context.file_content, 
            context.mode
        )
        
        # æ·»åŠ ç³»çµ±æç¤º
        system_prompt = self._get_system_prompt(context.mode)
        
        if was_truncated:
            system_prompt += "\n\næ³¨æ„ï¼šç”±æ–¼æª”æ¡ˆéå¤§ï¼Œåªæä¾›äº†éƒ¨åˆ†å…§å®¹é€²è¡Œåˆ†æã€‚è«‹åŸºæ–¼å¯è¦‹çš„å…§å®¹æä¾›åˆ†æã€‚"
        
        messages.append({"role": "system", "content": system_prompt})
        
        # æ·»åŠ æ­·å²è¨Šæ¯ï¼ˆé™åˆ¶æ•¸é‡ï¼‰
        recent_messages = context.previous_messages[-2:]  # Realtek æ¨¡å‹ä¿ç•™æ›´å°‘æ­·å²
        for msg in recent_messages:
            if msg.role == MessageRole.USER:
                content = msg.content[:800] if len(msg.content) > 800 else msg.content
                messages.append({"role": "user", "content": content})
            elif msg.role == MessageRole.ASSISTANT:
                content = msg.content[:1500] if len(msg.content) > 1500 else msg.content
                messages.append({"role": "assistant", "content": content})
        
        # æ·»åŠ ç•¶å‰è«‹æ±‚
        user_message = f"æª”æ¡ˆè·¯å¾‘: {context.file_path}\n\næª”æ¡ˆå…§å®¹:\n{truncated_content}"
        messages.append({"role": "user", "content": user_message})
        
        # å°‡æˆªå–ç‹€æ…‹ä¿å­˜åˆ°ä¸Šä¸‹æ–‡
        context.metadata['was_truncated'] = was_truncated
        context.metadata['original_size'] = len(context.file_content)
        context.metadata['truncated_size'] = len(truncated_content)
        context.metadata['actual_input_tokens'] = self.calculate_tokens(user_message)
        
        return messages
    
    def _get_system_prompt(self, mode: AnalysisMode) -> str:
        """æ ¹æ“šæ¨¡å¼ç²å–ç³»çµ±æç¤º"""
        prompts = {
            AnalysisMode.SMART: """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ Android ç³»çµ±æ—¥èªŒåˆ†æå°ˆå®¶ã€‚è«‹åˆ†ææä¾›çš„æ—¥èªŒå…§å®¹ï¼š

1. è‡ªå‹•è­˜åˆ¥æ—¥èªŒé¡å‹ï¼š
   - å¦‚æœåŒ…å« "Subject: Input dispatching timed out" æˆ–é¡ä¼¼å…§å®¹ï¼Œé€™æ˜¯ ANR (Application Not Responding) æ—¥èªŒ
   - å¦‚æœåŒ…å« "tombstone" æˆ–å´©æ½°å †ç–Šï¼Œé€™æ˜¯å´©æ½°æ—¥èªŒ
   - å…¶ä»–é¡å‹è«‹ç›¸æ‡‰è­˜åˆ¥

2. æ ¹æ“šæ—¥èªŒé¡å‹æä¾›åˆ†æï¼š
   - **å•é¡Œæ‘˜è¦**ï¼šç°¡æ½”æè¿°ç™¼ç”Ÿäº†ä»€éº¼å•é¡Œ
   - **æ ¹æœ¬åŸå› **ï¼šæ·±å…¥åˆ†æå°è‡´å•é¡Œçš„åŸå› 
   - **å½±éŸ¿ç¯„åœ**ï¼šå“ªäº›é€²ç¨‹/çµ„ä»¶å—åˆ°å½±éŸ¿
   - **è§£æ±ºæ–¹æ¡ˆ**ï¼šæä¾›å…·é«”çš„ä¿®å¾©å»ºè­°
   
3. è«‹ä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå’Œæ ¼å¼åŒ–è¼¸å‡ºï¼Œå›ç­”è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡""",
            
            AnalysisMode.QUICK: """ä½ æ˜¯ä¸€å€‹å¿«é€Ÿè¨ºæ–·åŠ©æ‰‹ã€‚è«‹å¿«é€Ÿåˆ†æé€™å€‹ Android æ—¥èªŒï¼š

å¿«é€Ÿè­˜åˆ¥ï¼š
- æ—¥èªŒé¡å‹ï¼ˆANR/å´©æ½°/å…¶ä»–ï¼‰
- ä¸»è¦å•é¡Œ
- ç«‹å³å¯åŸ·è¡Œçš„è§£æ±ºæ–¹æ¡ˆ

è«‹ä¿æŒç°¡æ½”ï¼Œåªæä¾›æœ€é—œéµçš„ä¿¡æ¯ï¼Œå›ç­”è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚""",
            
            AnalysisMode.DEEP: """ä½ æ˜¯ä¸€å€‹ç´°ç·»çš„ Android ç³»çµ±åˆ†æå°ˆå®¶ã€‚è«‹å°é€™å€‹æ—¥èªŒé€²è¡Œæ·±åº¦åˆ†æï¼š

1. **æ—¥èªŒè§£æ**
   - å®Œæ•´è§£è®€æ¯å€‹é—œéµä¿¡æ¯
   - åˆ†æè¨˜æ†¶é«”åœ°å€ã€æš«å­˜å™¨ç‹€æ…‹
   - è¿½è¹¤å®Œæ•´çš„èª¿ç”¨å †ç–Š

2. **å•é¡Œè¨ºæ–·**
   - è©³ç´°çš„æ ¹å› åˆ†æ
   - å¯èƒ½çš„è§¸ç™¼æ¢ä»¶
   - ç³»çµ±ç‹€æ…‹è©•ä¼°

3. **è§£æ±ºæ–¹æ¡ˆ**
   - ç¨‹å¼ç¢¼å±¤é¢çš„ä¿®å¾©å»ºè­°
   - ç³»çµ±é…ç½®å„ªåŒ–
   - é é˜²æªæ–½

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œæä¾›è©³ç´°ä¸”å°ˆæ¥­çš„åˆ†æã€‚"""
        }
        return prompts.get(mode, prompts[AnalysisMode.SMART])
    
    def _get_max_tokens(self, mode: AnalysisMode) -> int:
        """æ ¹æ“šæ¨¡å¼ç²å–æœ€å¤§è¼¸å‡º token æ•¸"""
        base_max = self.max_output_tokens
        
        # æ ¹æ“šæ¨¡å‹èª¿æ•´è¼¸å‡º token é™åˆ¶
        if self.model == 'chat-chattek-qwen':  # 256K æ¨¡å‹ï¼Œå¯ä»¥æ›´å¤šè¼¸å‡º
            tokens = {
                AnalysisMode.QUICK: min(1000, base_max),
                AnalysisMode.SMART: min(2000, base_max),
                AnalysisMode.DEEP: min(4000, base_max)
            }
        else:  # chat-chattek-gpt (128K æ¨¡å‹)
            tokens = {
                AnalysisMode.QUICK: min(800, base_max),
                AnalysisMode.SMART: min(1500, base_max),
                AnalysisMode.DEEP: min(3000, base_max)
            }
        return tokens.get(mode, min(1500, base_max))
    
    def _get_temperature(self, mode: AnalysisMode) -> float:
        """æ ¹æ“šæ¨¡å¼ç²å–æº«åº¦åƒæ•¸"""
        temps = {
            AnalysisMode.QUICK: 0.3,
            AnalysisMode.SMART: 0.5,
            AnalysisMode.DEEP: 0.7
        }
        return temps.get(mode, 0.5)

class AnthropicProvider(AIProvider):
    """Anthropic Claude Provider"""
    
    def __init__(self, api_key: str, model: str):
        super().__init__(api_key, model)
        self.client = anthropic.Anthropic(api_key=api_key)
        
        # å¾é…ç½®ç²å–æ¨¡å‹é™åˆ¶
        self.model_config = MODEL_LIMITS.get(model, {})
        self.max_tokens = self.model_config.get('max_tokens', 200000)
        self.max_output_tokens = self.model_config.get('max_output_tokens', 4096)
        self.chars_per_token = self.model_config.get('chars_per_token', 4)
    
    def calculate_tokens(self, text: str) -> int:
        """è¨ˆç®— token æ•¸é‡"""
        if not text:
            return 0
        return len(text) // self.chars_per_token
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è¨ˆç®—æˆæœ¬"""
        pricing = TOKEN_PRICING.get('anthropic', {}).get(self.model, {'input': 0, 'output': 0})
        input_cost = (input_tokens / 1000) * pricing.get('input', 0)
        output_cost = (output_tokens / 1000) * pricing.get('output', 0)
        return input_cost + output_cost
    
    def _truncate_content(self, content: str, mode: AnalysisMode) -> tuple[str, bool]:
        """æ ¹æ“š token é™åˆ¶æˆªå–å…§å®¹"""
        reserved_tokens = 5000
        
        if mode == AnalysisMode.QUICK:
            max_content_tokens = min(50000, self.max_tokens - reserved_tokens)
        elif mode == AnalysisMode.DEEP:
            max_content_tokens = self.max_tokens - reserved_tokens
        else:
            max_content_tokens = min(100000, self.max_tokens - reserved_tokens)
        
        current_tokens = self.calculate_tokens(content)
        
        if current_tokens <= max_content_tokens:
            return content, False
        
        max_chars = max_content_tokens * self.chars_per_token
        
        if mode == AnalysisMode.QUICK:
            head_chars = max_chars // 3
            tail_chars = max_chars // 3
            
            truncated = (
                content[:head_chars] + 
                f"\n\n... [çœç•¥ä¸­é–“å…§å®¹ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ] ...\n\n" +
                content[-tail_chars:]
            )
        else:
            truncated = content[:max_chars] + f"\n\n... [å…§å®¹å·²æˆªå–ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ]"
        
        return truncated, True
    
    def analyze_sync(self, context: AnalysisContext) -> str:
        """åŒæ­¥åˆ†æ"""
        try:
            messages = self._prepare_messages(context)
            response = self.client.messages.create(
                model=self.model,
                messages=messages,
                max_tokens=self._get_max_tokens(context.mode),
                temperature=self._get_temperature(context.mode)
            )
            return response.content[0].text
        except Exception as e:
            raise Exception(f"Anthropic API éŒ¯èª¤: {str(e)}")
    
    def stream_analyze_sync(self, context: AnalysisContext):
        """åŒæ­¥æµå¼åˆ†æ"""
        self.reset_stop_flag()
        try:
            messages = self._prepare_messages(context)
            
            with self.client.messages.stream(
                model=self.model,
                messages=messages,
                max_tokens=self._get_max_tokens(context.mode),
                temperature=self._get_temperature(context.mode)
            ) as stream:
                for text in stream.text_stream:
                    if self.should_stop():
                        break
                    yield text
                    
        except Exception as e:
            yield f"\n\néŒ¯èª¤: {str(e)}"
    
    def _prepare_messages(self, context: AnalysisContext) -> List[Dict]:
        """æº–å‚™è¨Šæ¯æ ¼å¼"""
        messages = []
        
        truncated_content, was_truncated = self._truncate_content(
            context.file_content, 
            context.mode
        )
        
        system_prompt = self._get_system_prompt(context.mode)
        
        if was_truncated:
            system_prompt += "\n\næ³¨æ„ï¼šç”±æ–¼æª”æ¡ˆéå¤§ï¼Œåªæä¾›äº†éƒ¨åˆ†å…§å®¹é€²è¡Œåˆ†æã€‚è«‹åŸºæ–¼å¯è¦‹çš„å…§å®¹æä¾›åˆ†æã€‚"
        
        recent_messages = context.previous_messages[-3:]
        for msg in recent_messages:
            if msg.role == MessageRole.USER:
                content = msg.content[:1000] if len(msg.content) > 1000 else msg.content
                messages.append({"role": "user", "content": content})
            elif msg.role == MessageRole.ASSISTANT:
                content = msg.content[:2000] if len(msg.content) > 2000 else msg.content
                messages.append({"role": "assistant", "content": content})
        
        user_message = f"{system_prompt}\n\næª”æ¡ˆè·¯å¾‘: {context.file_path}\n\næª”æ¡ˆå…§å®¹:\n{truncated_content}"
        messages.append({"role": "user", "content": user_message})
        
        context.metadata['was_truncated'] = was_truncated
        context.metadata['original_size'] = len(context.file_content)
        context.metadata['truncated_size'] = len(truncated_content)
        context.metadata['actual_input_tokens'] = self.calculate_tokens(user_message)
        
        return messages
    
    def _get_system_prompt(self, mode: AnalysisMode) -> str:
        """æ ¹æ“šæ¨¡å¼ç²å–ç³»çµ±æç¤º"""
        prompts = {
            AnalysisMode.SMART: """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ Android ç³»çµ±æ—¥èªŒåˆ†æå°ˆå®¶ã€‚è«‹åˆ†ææä¾›çš„æ—¥èªŒå…§å®¹ï¼š

1. è‡ªå‹•è­˜åˆ¥æ—¥èªŒé¡å‹ï¼š
   - å¦‚æœåŒ…å« "Subject: Input dispatching timed out" æˆ–é¡ä¼¼å…§å®¹ï¼Œé€™æ˜¯ ANR (Application Not Responding) æ—¥èªŒ
   - å¦‚æœåŒ…å« "tombstone" æˆ–å´©æ½°å †ç–Šï¼Œé€™æ˜¯å´©æ½°æ—¥èªŒ
   - å…¶ä»–é¡å‹è«‹ç›¸æ‡‰è­˜åˆ¥

2. æ ¹æ“šæ—¥èªŒé¡å‹æä¾›åˆ†æï¼š
   - **å•é¡Œæ‘˜è¦**ï¼šç°¡æ½”æè¿°ç™¼ç”Ÿäº†ä»€éº¼å•é¡Œ
   - **æ ¹æœ¬åŸå› **ï¼šæ·±å…¥åˆ†æå°è‡´å•é¡Œçš„åŸå› 
   - **å½±éŸ¿ç¯„åœ**ï¼šå“ªäº›é€²ç¨‹/çµ„ä»¶å—åˆ°å½±éŸ¿
   - **è§£æ±ºæ–¹æ¡ˆ**ï¼šæä¾›å…·é«”çš„ä¿®å¾©å»ºè­°
   
3. è«‹ä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå’Œæ ¼å¼åŒ–è¼¸å‡º""",
            
            AnalysisMode.QUICK: """ä½ æ˜¯ä¸€å€‹å¿«é€Ÿè¨ºæ–·åŠ©æ‰‹ã€‚åœ¨ 30 ç§’å…§åˆ†æé€™å€‹ Android æ—¥èªŒï¼š

å¿«é€Ÿè­˜åˆ¥ï¼š
- æ—¥èªŒé¡å‹ï¼ˆANR/å´©æ½°/å…¶ä»–ï¼‰
- ä¸»è¦å•é¡Œ
- ç«‹å³å¯åŸ·è¡Œçš„è§£æ±ºæ–¹æ¡ˆ

è«‹ä¿æŒç°¡æ½”ï¼Œåªæä¾›æœ€é—œéµçš„ä¿¡æ¯ã€‚""",
            
            AnalysisMode.DEEP: """ä½ æ˜¯ä¸€å€‹ç´°ç·»çš„ Android ç³»çµ±åˆ†æå°ˆå®¶ã€‚è«‹å°é€™å€‹æ—¥èªŒé€²è¡Œæ·±åº¦åˆ†æï¼š

1. **æ—¥èªŒè§£æ**
   - å®Œæ•´è§£è®€æ¯å€‹é—œéµä¿¡æ¯
   - åˆ†æè¨˜æ†¶é«”åœ°å€ã€æš«å­˜å™¨ç‹€æ…‹
   - è¿½è¹¤å®Œæ•´çš„èª¿ç”¨å †ç–Š

2. **å•é¡Œè¨ºæ–·**
   - è©³ç´°çš„æ ¹å› åˆ†æ
   - å¯èƒ½çš„è§¸ç™¼æ¢ä»¶
   - ç³»çµ±ç‹€æ…‹è©•ä¼°

3. **è§£æ±ºæ–¹æ¡ˆ**
   - ç¨‹å¼ç¢¼å±¤é¢çš„ä¿®å¾©å»ºè­°
   - ç³»çµ±é…ç½®å„ªåŒ–
   - é é˜²æªæ–½

4. **ç›¸é—œçŸ¥è­˜**
   - ç›¸é—œçš„ Android ç³»çµ±çŸ¥è­˜
   - é¡ä¼¼å•é¡Œçš„è™•ç†ç¶“é©—"""
        }
        return prompts.get(mode, prompts[AnalysisMode.SMART])
    
    def _get_max_tokens(self, mode: AnalysisMode) -> int:
        """æ ¹æ“šæ¨¡å¼ç²å–æœ€å¤§è¼¸å‡º token æ•¸"""
        base_max = self.max_output_tokens
        
        tokens = {
            AnalysisMode.QUICK: min(1000, base_max),
            AnalysisMode.SMART: min(2000, base_max),
            AnalysisMode.DEEP: min(4000, base_max)
        }
        return tokens.get(mode, min(2000, base_max))
    
    def _get_temperature(self, mode: AnalysisMode) -> float:
        """æ ¹æ“šæ¨¡å¼ç²å–æº«åº¦åƒæ•¸"""
        temps = {
            AnalysisMode.QUICK: 0.3,
            AnalysisMode.SMART: 0.5,
            AnalysisMode.DEEP: 0.7
        }
        return temps.get(mode, 0.5)

class ProviderFactory:
    """AI Provider å·¥å» é¡"""
    
    @staticmethod
    def create_provider(provider_name: str, model: str) -> AIProvider:
        """å‰µå»º AI Provider å¯¦ä¾‹"""
        if provider_name not in AI_PROVIDERS:
            raise ValueError(f"ä¸æ”¯æ´çš„ Provider: {provider_name}")
        
        config = AI_PROVIDERS[provider_name]
        api_key = config['api_key']
        
        if not api_key:
            raise ValueError(f"{provider_name} API Key æœªè¨­ç½®")
        
        if provider_name == 'anthropic':
            return AnthropicProvider(api_key, model)
        elif provider_name == 'realtek':
            base_url = config.get('base_url', REALTEK_BASE_URL)
            return RealtekProvider(api_key, model, base_url)
        elif provider_name == 'openai':
            # å¦‚æœæ‚¨éœ€è¦ OpenAI æ”¯æŒï¼Œå¯ä»¥åœ¨é€™è£¡å¯¦ç¾
            raise ValueError("OpenAI Provider å°šæœªå¯¦ç¾")
        else:
            raise ValueError(f"æœªå¯¦ä½œçš„ Provider: {provider_name}")

class AnalysisSession:
    """åˆ†ææœƒè©±ç®¡ç†"""
    
    def __init__(self, session_id: str, provider: AIProvider):
        self.session_id = session_id
        self.provider = provider
        self.messages: List[Message] = []
        self.created_at = datetime.now()
        self.last_activity = datetime.now()
        self.is_active = True
        self.total_cost = 0.0
        self.total_tokens = {'input': 0, 'output': 0}
        
    def add_message(self, role: MessageRole, content: str):
        """æ·»åŠ è¨Šæ¯åˆ°æœƒè©±"""
        self.messages.append(Message(role, content))
        self.last_activity = datetime.now()
        
    def stop(self):
        """åœæ­¢ç•¶å‰åˆ†æ"""
        self.provider.stop()
        self.is_active = False
        
    def update_usage(self, input_tokens: int, output_tokens: int):
        """æ›´æ–°ä½¿ç”¨é‡çµ±è¨ˆ"""
        self.total_tokens['input'] += input_tokens
        self.total_tokens['output'] += output_tokens
        self.total_cost += self.provider.get_cost(input_tokens, output_tokens)

# API è·¯ç”±
@ai_analyzer_bp.route('/api/ai/analyze', methods=['POST'])
def analyze_file():
    """åŸ·è¡Œ AI åˆ†æ"""
    try:
        data = request.json
        session_id = data.get('session_id', str(time.time()))
        provider_name = data.get('provider', 'realtek')  # é è¨­ä½¿ç”¨ Realtek
        model = data.get('model', AI_PROVIDERS[provider_name]['default_model'])
        mode = AnalysisMode(data.get('mode', 'smart'))
        file_path = data.get('file_path', '')
        file_name = data.get('file_name', '')
        file_content = data.get('content', '')
        stream = data.get('stream', True)
        context_messages = data.get('context', [])
        
        # å‰µå»ºæˆ–ç²å–æœƒè©±
        if session_id in active_analyses:
            session = active_analyses[session_id]
        else:
            provider = ProviderFactory.create_provider(provider_name, model)
            session = AnalysisSession(session_id, provider)
            active_analyses[session_id] = session
        
        # æ·»åŠ ä¸Šä¸‹æ–‡è¨Šæ¯åˆ°æœƒè©±
        for msg in context_messages:
            if 'role' in msg and 'content' in msg:
                try:
                    role = MessageRole(msg['role']) if msg['role'] in ['user', 'assistant', 'system'] else MessageRole.USER
                    session.add_message(role, msg['content'])
                except ValueError:
                    session.add_message(MessageRole.USER, msg['content'])
        
        # æº–å‚™åˆ†æä¸Šä¸‹æ–‡
        context = AnalysisContext(
            file_path=file_path,
            file_content=file_content,
            mode=mode,
            previous_messages=session.messages,
            metadata={
                'session_id': session_id,
                'file_name': file_name
            }
        )
        
        if stream:
            def generate():
                """ç”Ÿæˆ SSE æµ"""
                try:
                    # ç™¼é€é–‹å§‹äº‹ä»¶
                    yield f"data: {json.dumps({'type': 'start', 'mode': mode.value})}\n\n"
                    
                    # æª¢æŸ¥æª”æ¡ˆå¤§å°å’Œæ¨¡å‹é¡å‹
                    file_size = len(file_content)
                    estimated_tokens = session.provider.calculate_tokens(file_content)
                    
                    # æ ¹æ“šä¸åŒ Provider é¡¯ç¤ºä¸åŒè³‡è¨Š
                    if provider_name == 'realtek':
                        file_info = f"æ­£åœ¨ä½¿ç”¨ Realtek å…§éƒ¨æ¨¡å‹åˆ†æ: {file_name or file_path}"
                    else:
                        file_info = f"æ­£åœ¨åˆ†ææª”æ¡ˆ: {file_name or file_path}"
                    
                    # è­˜åˆ¥æª”æ¡ˆé¡å‹
                    if 'tombstone' in file_path.lower() or 'tombstone' in file_content[:1000].lower():
                        file_info += " (Tombstone å´©æ½°æ—¥èªŒ)"
                    elif 'anr' in file_path.lower() or 'input dispatching timed out' in file_content[:5000].lower():
                        file_info += " (ANR æ—¥èªŒ)"
                    elif 'fatal' in file_content[:1000].lower() or 'crash' in file_content[:1000].lower():
                        file_info += " (å´©æ½°æ—¥èªŒ)"
                    
                    yield f"data: {json.dumps({'type': 'info', 'message': file_info})}\n\n"
                    
                    # é¡¯ç¤ºæª”æ¡ˆå¤§å°è³‡è¨Š
                    size_info = f"æª”æ¡ˆå¤§å°: {file_size:,} å­—å…ƒ (ç´„ {estimated_tokens:,} tokens)"
                    yield f"data: {json.dumps({'type': 'info', 'message': size_info})}\n\n"
                    
                    # æ ¹æ“šä¸åŒæ¨¡å‹é¡¯ç¤ºä¸åŒçš„ token é™åˆ¶è­¦å‘Š
                    model_config = MODEL_LIMITS.get(model, {})
                    max_tokens = model_config.get('max_tokens', 200000)
                    
                    if estimated_tokens > max_tokens * 0.8:
                        if provider_name == 'realtek':
                            warning_msg = f"âš ï¸ æª”æ¡ˆè¼ƒå¤§ï¼ˆç´„ {estimated_tokens:,} tokensï¼‰ï¼ŒRealtek æ¨¡å‹å°‡è‡ªå‹•æˆªå–é©ç•¶å…§å®¹"
                        else:
                            warning_msg = f"âš ï¸ æª”æ¡ˆè¼ƒå¤§ï¼ˆç´„ {estimated_tokens:,} tokensï¼‰ï¼Œå°‡è‡ªå‹•æˆªå–é©ç•¶å…§å®¹é€²è¡Œåˆ†æ"
                        yield f"data: {json.dumps({'type': 'warning', 'message': warning_msg})}\n\n"
                    
                    # æ ¹æ“šæ¨¡å¼é¡¯ç¤ºé æœŸæ™‚é–“
                    if mode == AnalysisMode.QUICK:
                        if provider_name == 'realtek':
                            yield f"data: {json.dumps({'type': 'info', 'message': 'âš¡ Realtek å¿«é€Ÿåˆ†ææ¨¡å¼ï¼šé è¨ˆ 20 ç§’å…§å®Œæˆ'})}\n\n"
                        else:
                            yield f"data: {json.dumps({'type': 'info', 'message': 'âš¡ å¿«é€Ÿåˆ†ææ¨¡å¼ï¼šé è¨ˆ 30 ç§’å…§å®Œæˆ'})}\n\n"
                    elif mode == AnalysisMode.DEEP:
                        yield f"data: {json.dumps({'type': 'info', 'message': 'ğŸ” æ·±åº¦åˆ†ææ¨¡å¼ï¼šé è¨ˆ 2-3 åˆ†é˜å®Œæˆ'})}\n\n"
                    else:
                        yield f"data: {json.dumps({'type': 'info', 'message': 'ğŸ§  æ™ºèƒ½åˆ†ææ¨¡å¼ï¼šè‡ªå‹•å¹³è¡¡é€Ÿåº¦èˆ‡æ·±åº¦'})}\n\n"
                    
                    # ä¼°ç®—è¼¸å…¥ tokens
                    yield f"data: {json.dumps({'type': 'tokens', 'input': estimated_tokens})}\n\n"
                    
                    # æµå¼è¼¸å‡º
                    output_content = ""
                    chunk_count = 0
                    last_update_time = time.time()
                    
                    try:
                        for chunk in session.provider.stream_analyze_sync(context):
                            # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                            if not session.is_active:
                                yield f"data: {json.dumps({'type': 'stopped', 'message': 'åˆ†æå·²è¢«ä½¿ç”¨è€…ä¸­æ–·'})}\n\n"
                                break
                            
                            output_content += chunk
                            chunk_count += 1
                            
                            # ç™¼é€å…§å®¹
                            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
                            
                            # å®šæœŸæ›´æ–° token è¨ˆæ•¸
                            current_time = time.time()
                            if current_time - last_update_time > 1.0:
                                output_tokens = session.provider.calculate_tokens(output_content)
                                yield f"data: {json.dumps({'type': 'tokens', 'output': output_tokens})}\n\n"
                                last_update_time = current_time
                    
                    except Exception as stream_error:
                        # è™•ç†æµå¼è¼¸å‡ºä¸­çš„éŒ¯èª¤
                        error_msg = str(stream_error)
                        
                        # é‡å°ä¸åŒ Provider æä¾›ä¸åŒçš„éŒ¯èª¤è™•ç†
                        if provider_name == 'realtek':
                            if "rate limit" in error_msg.lower():
                                error_msg = "Realtek API è«‹æ±‚é »ç¹ï¼Œè«‹ç¨å¾Œå†è©¦"
                            elif "prompt is too long" in error_msg.lower():
                                error_msg = "æª”æ¡ˆå…§å®¹éå¤§ï¼Œè«‹å˜—è©¦ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼"
                        else:
                            if "rate limit" in error_msg.lower():
                                error_msg = "è¶…é API é€Ÿç‡é™åˆ¶ï¼Œè«‹ç¨å¾Œå†è©¦ï¼ˆå»ºè­°ç­‰å¾… 30 ç§’ï¼‰"
                            elif "prompt is too long" in error_msg.lower():
                                error_msg = "æª”æ¡ˆå…§å®¹éå¤§ï¼Œè«‹å˜—è©¦ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼æˆ–åˆ†æ®µåˆ†æ"
                        
                        yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
                        return
                    
                    # å¦‚æœå…§å®¹è¢«æˆªå–ï¼Œåœ¨å®Œæˆæ™‚é€šçŸ¥
                    if context.metadata.get('was_truncated', False):
                        original_size = context.metadata.get('original_size', 0)
                        truncated_size = context.metadata.get('truncated_size', 0)
                        truncate_percentage = (truncated_size / original_size * 100) if original_size > 0 else 0
                        
                        truncate_info = {
                            'type': 'info',
                            'message': f"âœ‚ï¸ åˆ†æå®Œæˆã€‚ç”±æ–¼æª”æ¡ˆéå¤§ï¼Œå¯¦éš›åˆ†æäº† {truncated_size:,} / {original_size:,} å­—å…ƒ ({truncate_percentage:.1f}%)"
                        }
                        yield f"data: {json.dumps(truncate_info)}\n\n"
                    
                    # å®Œæˆåˆ†æ
                    if session.is_active and output_content:
                        # æ·»åŠ è¨Šæ¯åˆ°æœƒè©±æ­·å²
                        session.add_message(MessageRole.USER, f"åˆ†æ {mode.value} æ¨¡å¼: {file_name or file_path}")
                        session.add_message(MessageRole.ASSISTANT, output_content)
                        
                        # è¨ˆç®—æœ€çµ‚çš„ token ä½¿ç”¨é‡
                        final_output_tokens = session.provider.calculate_tokens(output_content)
                        actual_input_tokens = context.metadata.get('actual_input_tokens', estimated_tokens)
                        session.update_usage(actual_input_tokens, final_output_tokens)
                        
                        # ç™¼é€å®Œæˆäº‹ä»¶
                        yield f"data: {json.dumps({
                            'type': 'complete',
                            'usage': {
                                'input': session.total_tokens['input'],
                                'output': session.total_tokens['output'],
                                'total': session.total_tokens['input'] + session.total_tokens['output']
                            },
                            'cost': session.total_cost,
                            'was_truncated': context.metadata.get('was_truncated', False),
                            'duration': (datetime.now() - session.created_at).total_seconds(),
                            'provider': provider_name,
                            'model': model
                        })}\n\n"
                    
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    
                    error_msg = str(e)
                    # é‡å°ä¸åŒ Provider æä¾›æ›´å‹å¥½çš„éŒ¯èª¤è¨Šæ¯
                    if provider_name == 'realtek':
                        if "Connection" in error_msg or "timeout" in error_msg.lower():
                            error_msg = "Realtek å…§éƒ¨ API é€£ç·šè¶…æ™‚ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£æ¥"
                        elif "api key" in error_msg.lower():
                            error_msg = "Realtek API Key ç„¡æ•ˆæˆ–éæœŸï¼Œè«‹è¯ç¹«ç®¡ç†å“¡"
                    else:
                        if "prompt is too long" in error_msg:
                            error_msg = "æª”æ¡ˆå…§å®¹è¶…éæ¨¡å‹é™åˆ¶ï¼Œè«‹ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼æˆ–æ¸›å°‘æª”æ¡ˆå¤§å°"
                        elif "rate limit" in error_msg.lower():
                            error_msg = "API è«‹æ±‚éæ–¼é »ç¹ï¼Œè«‹ç­‰å¾… 30 ç§’å¾Œå†è©¦"
                        elif "api key" in error_msg.lower():
                            error_msg = "API Key ç„¡æ•ˆæˆ–æœªè¨­ç½®ï¼Œè«‹æª¢æŸ¥é…ç½®"
                    
                    yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
            
            return Response(
                stream_with_context(generate()),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no',
                    'Connection': 'keep-alive',
                    'Content-Type': 'text/event-stream'
                }
            )
        else:
            # éæµå¼å›æ‡‰
            try:
                result = session.provider.analyze_sync(context)
                
                was_truncated = context.metadata.get('was_truncated', False)
                
                session.add_message(MessageRole.USER, f"åˆ†ææª”æ¡ˆ: {file_name or file_path}")
                session.add_message(MessageRole.ASSISTANT, result)
                
                input_tokens = session.provider.calculate_tokens(file_content)
                output_tokens = session.provider.calculate_tokens(result)
                session.update_usage(input_tokens, output_tokens)
                
                response_data = {
                    'success': True,
                    'result': result,
                    'session_id': session_id,
                    'usage': {
                        'input': session.total_tokens['input'],
                        'output': session.total_tokens['output'],
                        'total': session.total_tokens['input'] + session.total_tokens['output']
                    },
                    'cost': session.total_cost,
                    'was_truncated': was_truncated,
                    'provider': provider_name,
                    'model': model
                }
                
                if was_truncated:
                    response_data['truncation_info'] = {
                        'original_size': context.metadata.get('original_size', 0),
                        'truncated_size': context.metadata.get('truncated_size', 0)
                    }
                
                return jsonify(response_data)
                
            except Exception as e:
                import traceback
                traceback.print_exc()
                
                error_msg = str(e)
                if provider_name == 'realtek':
                    if "Connection" in error_msg:
                        error_msg = "Realtek å…§éƒ¨ API é€£ç·šå¤±æ•—"
                else:
                    if "prompt is too long" in error_msg:
                        error_msg = "æª”æ¡ˆå…§å®¹è¶…éæ¨¡å‹é™åˆ¶"
                    elif "rate limit" in error_msg.lower():
                        error_msg = "API è«‹æ±‚é™åˆ¶"
                
                return jsonify({
                    'success': False,
                    'error': error_msg,
                    'details': str(e),
                    'provider': provider_name
                }), 500
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'error': 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤',
            'details': str(e)
        }), 500

@ai_analyzer_bp.route('/api/ai/stop/<session_id>', methods=['POST'])
def stop_analysis(session_id):
    """åœæ­¢åˆ†æ"""
    if session_id in active_analyses:
        session = active_analyses[session_id]
        session.stop()
        return jsonify({'success': True, 'message': 'åˆ†æå·²åœæ­¢'})
    return jsonify({'success': False, 'error': 'æœƒè©±ä¸å­˜åœ¨'}), 404

@ai_analyzer_bp.route('/api/ai/models/<provider>', methods=['GET'])
def get_models(provider):
    """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    if provider not in AI_PROVIDERS:
        return jsonify({'error': 'ä¸æ”¯æ´çš„ Provider'}), 400
        
    models = AI_PROVIDERS[provider]['models']
    return jsonify({
        'models': [
            {
                'id': model_id,
                'name': info['name'],
                'description': info['description'],
                'max_tokens': info['max_tokens'],
                'pricing': TOKEN_PRICING.get(provider, {}).get(model_id, {}),
                'provider': provider
            }
            for model_id, info in models.items()
        ]
    })

# åœ¨ routes/ai_analyzer.py ä¸­æ·»åŠ 
@ai_analyzer_bp.route('/api/ai/segment-analyze', methods=['POST'])
def segment_analyze():
    """åˆ†æ®µåˆ†æ API - æ”¯æŒæ¯æ®µ streaming è¼¸å‡º"""
    import time
    from datetime import datetime
    
    try:
        data = request.json
        session_id = data.get('session_id', str(time.time()))
        provider_name = data.get('provider', 'realtek')
        model = data.get('model', AI_PROVIDERS[provider_name]['default_model'])
        mode = AnalysisMode(data.get('mode', 'deep'))
        file_path = data.get('file_path', '')
        file_name = data.get('file_name', '')
        file_content = data.get('content', '')
        
        print(f"é–‹å§‹åˆ†æ®µåˆ†æ - å…§å®¹é•·åº¦: {len(file_content)}")
        
        # å‰µå»º Provider
        provider = ProviderFactory.create_provider(provider_name, model)
        
        # è¨ˆç®—åˆ†æ®µç­–ç•¥
        segments = create_smart_segments(file_content, provider, mode)
        total_segments = len(segments)
        
        print(f"åˆ†æ®µç­–ç•¥: {total_segments} æ®µ")
        
        def generate_segments():
            """ç”Ÿæˆåˆ†æ®µåˆ†æçµæœ - æ¯æ®µæ”¯æŒ streaming"""
            try:
                # ç™¼é€é–‹å§‹äº‹ä»¶
                yield f"data: {json.dumps({'type': 'segment_start', 'total_segments': total_segments, 'mode': mode.value})}\n\n"
                
                segment_results = []
                
                for i, segment in enumerate(segments):
                    segment_num = i + 1
                    print(f"è™•ç†æ®µè½ {segment_num}/{total_segments}")
                    
                    # ç™¼é€é€²åº¦æ›´æ–°
                    progress_data = {
                        'type': 'segment_progress',
                        'current_segment': segment_num,
                        'total_segments': total_segments,
                        'progress': int((segment_num - 1) / total_segments * 100),
                        'message': f'æ­£åœ¨åˆ†æç¬¬ {segment_num} æ®µï¼ˆå…± {total_segments} æ®µï¼‰'
                    }
                    yield f"data: {json.dumps(progress_data)}\n\n"
                    
                    # ç™¼é€æ®µè½é–‹å§‹äº‹ä»¶
                    yield f"data: {json.dumps({'type': 'segment_content_start', 'segment_number': segment_num})}\n\n"
                    
                    try:
                        # å‰µå»ºå–®æ®µåˆ†æä¸Šä¸‹æ–‡
                        segment_context = AnalysisContext(
                            file_path=file_path,
                            file_content=segment['content'],
                            mode=mode,
                            previous_messages=[],
                            metadata={
                                'segment_number': segment_num,
                                'total_segments': total_segments,
                                'segment_range': segment['range'],
                                'is_segment': True
                            }
                        )
                        
                        # ğŸ”¥ é—œéµæ”¹é€²ï¼šä½¿ç”¨ streaming åˆ†ææ¯å€‹æ®µè½
                        segment_content = ""
                        
                        # é€æ­¥è¼¸å‡ºæ¯æ®µçš„å…§å®¹
                        for chunk in provider.stream_analyze_sync(segment_context):
                            segment_content += chunk
                            # å¯¦æ™‚ç™¼é€æ¯æ®µçš„ streaming å…§å®¹
                            yield f"data: {json.dumps({'type': 'segment_content_chunk', 'segment_number': segment_num, 'content': chunk})}\n\n"
                        
                        # æ®µè½å®Œæˆ
                        segment_data = {
                            'segment_number': segment_num,
                            'range': segment['range'],
                            'content_length': len(segment['content']),
                            'analysis': segment_content,
                            'success': True
                        }
                        
                        segment_results.append(segment_data)
                        
                        # ç™¼é€æ®µè½å®Œæˆäº‹ä»¶
                        yield f"data: {json.dumps({'type': 'segment_complete', 'segment': segment_data})}\n\n"
                        
                    except Exception as segment_error:
                        print(f"æ®µè½ {segment_num} åˆ†æå¤±æ•—: {str(segment_error)}")
                        
                        segment_data = {
                            'segment_number': segment_num,
                            'range': segment['range'],
                            'error': str(segment_error),
                            'success': False
                        }
                        
                        segment_results.append(segment_data)
                        yield f"data: {json.dumps({'type': 'segment_error', 'segment': segment_data})}\n\n"
                    
                    # æ®µè½é–“å»¶é²ï¼ˆé¿å…é€Ÿç‡é™åˆ¶ï¼‰
                    if segment_num < total_segments:
                        time.sleep(1)  # æ¸›å°‘åˆ°1ç§’å»¶é²
                
                # ç”Ÿæˆç¶œåˆåˆ†æ - ä¹Ÿä½¿ç”¨ streaming
                print("ç”Ÿæˆç¶œåˆåˆ†æ...")
                yield f"data: {json.dumps({'type': 'generating_summary', 'message': 'æ­£åœ¨ç”Ÿæˆç¶œåˆåˆ†æ...', 'streaming': True})}\n\n"
                
                # ğŸ”¥ ç¶œåˆåˆ†æä¹Ÿæ”¯æŒ streaming
                final_analysis = ""
                try:
                    for chunk in generate_comprehensive_analysis_streaming(segment_results, file_path, provider, mode):
                        final_analysis += chunk
                        # ç™¼é€ç¶œåˆåˆ†æçš„ streaming å…§å®¹
                        yield f"data: {json.dumps({'type': 'final_analysis_chunk', 'content': chunk})}\n\n"
                except Exception as e:
                    print(f"ç¶œåˆåˆ†æå¤±æ•—: {str(e)}")
                    final_analysis = generate_comprehensive_analysis(segment_results, file_path, provider, mode)
                
                # ç™¼é€æœ€çµ‚çµæœ
                final_data = {
                    'type': 'segment_analysis_complete',
                    'total_segments': total_segments,
                    'successful_segments': len([s for s in segment_results if s.get('success')]),
                    'final_analysis': final_analysis,
                    'segment_results': segment_results
                }
                yield f"data: {json.dumps(final_data)}\n\n"
                
            except Exception as e:
                print(f"åˆ†æ®µåˆ†æéŒ¯èª¤: {str(e)}")
                yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
        
        return Response(
            stream_with_context(generate_segments()),
            mimetype='text/event-stream',
            headers={
                'Cache-Control': 'no-cache',
                'X-Accel-Buffering': 'no',
                'Connection': 'keep-alive',
                'Content-Type': 'text/event-stream'
            }
        )
        
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

def generate_comprehensive_analysis_streaming(segment_results, file_path, provider, mode):
    """ç”Ÿæˆç¶œåˆåˆ†æ - streaming ç‰ˆæœ¬"""
    successful_results = [s for s in segment_results if s.get('success') and s.get('analysis')]
    
    if not successful_results:
        yield "ç„¡æ³•ç”Ÿæˆç¶œåˆåˆ†æï¼šæ‰€æœ‰æ®µè½åˆ†æéƒ½å¤±æ•—äº†ã€‚"
        return
    
    # åˆä½µæ‰€æœ‰æˆåŠŸçš„åˆ†æçµæœ
    all_analyses = [s['analysis'] for s in successful_results]
    
    # å‰µå»ºç¶œåˆåˆ†ææç¤ºè©
    synthesis_prompt = f"""
åŸºæ–¼ä»¥ä¸‹ {len(all_analyses)} å€‹æ®µè½çš„åˆ†æçµæœï¼Œè«‹ç”Ÿæˆä¸€å€‹ç¶œåˆçš„åˆ†æå ±å‘Šï¼š

æª”æ¡ˆï¼š{file_path}
åˆ†ææ¨¡å¼ï¼š{mode.value}

æ®µè½åˆ†æçµæœï¼š
{chr(10).join([f"æ®µè½ {i+1}: {analysis[:500]}..." for i, analysis in enumerate(all_analyses)])}

è«‹æä¾›ï¼š
1. ç¶œåˆå•é¡Œæ‘˜è¦
2. ä¸»è¦ç™¼ç¾å’Œæ ¹æœ¬åŸå› 
3. å½±éŸ¿ç¯„åœè©•ä¼°
4. çµ±ä¸€çš„è§£æ±ºæ–¹æ¡ˆå»ºè­°
5. é˜²ç¯„æªæ–½

è«‹ç¢ºä¿åˆ†æçµæœé€£è²«ä¸”å®Œæ•´ï¼Œé¿å…é‡è¤‡å…§å®¹ã€‚
"""
    
    try:
        # ä½¿ç”¨ provider ç”Ÿæˆç¶œåˆåˆ†æ - streaming æ–¹å¼
        context = AnalysisContext(
            file_path=file_path,
            file_content=synthesis_prompt,
            mode=mode,
            previous_messages=[],
            metadata={'is_synthesis': True}
        )
        
        # ä½¿ç”¨ streaming æ–¹å¼è¼¸å‡ºç¶œåˆåˆ†æ
        for chunk in provider.stream_analyze_sync(context):
            yield chunk
            
    except Exception as e:
        print(f"ç¶œåˆåˆ†æ streaming å¤±æ•—: {str(e)}")
        # é™ç´šåˆ°åŸä¾†çš„åŒæ­¥æ–¹å¼
        yield generate_comprehensive_analysis(segment_results, file_path, provider, mode)

def create_smart_segments(content, provider, mode):
    """æ™ºèƒ½åˆ†æ®µç­–ç•¥"""
    # æ ¹æ“šæ¨¡å¼å’Œ provider ç¢ºå®šæ¯æ®µæœ€å¤§å¤§å°
    if hasattr(provider, 'model') and provider.model == 'chat-chattek-qwen':
        max_tokens_per_segment = 40000  # 256K æ¨¡å‹
    else:
        max_tokens_per_segment = 25000  # 128K æ¨¡å‹
    
    if mode == AnalysisMode.DEEP:
        max_tokens_per_segment = int(max_tokens_per_segment * 0.6)  # æ·±åº¦åˆ†æä½¿ç”¨æ›´å°çš„æ®µ
    
    chars_per_token = getattr(provider, 'chars_per_token', 2.5)
    max_chars_per_segment = int(max_tokens_per_segment * chars_per_token)
    
    segments = []
    
    # å˜—è©¦æŒ‰æ—¥èªŒé‚Šç•Œåˆ†æ®µ
    if is_log_file(content):
        segments = split_by_log_boundaries(content, max_chars_per_segment)
    else:
        # æŒ‰å›ºå®šå¤§å°åˆ†æ®µï¼ˆå¸¶é‡ç–Šï¼‰
        segments = split_by_size_with_overlap(content, max_chars_per_segment)
    
    print(f"åˆ†æ®µçµæœ: {len(segments)} æ®µ, æœ€å¤§æ®µè½: {max(len(s['content']) for s in segments)} å­—å…ƒ")
    
    return segments

def is_log_file(content):
    """æª¢æ¸¬æ˜¯å¦ç‚ºæ—¥èªŒæª”æ¡ˆ"""
    log_indicators = ['tombstone', 'anr', 'crash', 'backtrace', 'stack trace', 'fatal']
    content_lower = content[:5000].lower()
    return any(indicator in content_lower for indicator in log_indicators)

def split_by_log_boundaries(content, max_chars_per_segment):
    """æŒ‰æ—¥èªŒé‚Šç•Œåˆ†æ®µ"""
    segments = []
    
    # å˜—è©¦æ‰¾åˆ°è‡ªç„¶çš„åˆ†æ®µé»
    separators = [
        '\n*** *** ***',  # Tombstone åˆ†éš”ç¬¦
        '\nANR in ',      # ANR é–‹å§‹
        '\nmain" prio=',  # ä¸»ç·šç¨‹
        '\n"Thread-',     # ç·šç¨‹é–‹å§‹
        '\nBacktrace:',   # å †æ£§é–‹å§‹
        '\n\n\n',         # ä¸‰å€‹ç©ºè¡Œ
    ]
    
    lines = content.split('\n')
    current_segment = []
    current_size = 0
    
    for line in lines:
        line_size = len(line) + 1  # +1 for newline
        
        # æª¢æŸ¥æ˜¯å¦åˆ°é”åˆ†æ®µé‚Šç•Œ
        should_split = False
        if current_size + line_size > max_chars_per_segment:
            should_split = True
        else:
            for sep in separators:
                if line.startswith(sep.strip()):
                    if current_size > max_chars_per_segment * 0.3:  # è‡³å°‘ 30% æ‰åˆ†æ®µ
                        should_split = True
                    break
        
        if should_split and current_segment:
            segment_content = '\n'.join(current_segment)
            segments.append({
                'content': segment_content,
                'range': f'{len(segments) * max_chars_per_segment}-{len(segments) * max_chars_per_segment + len(segment_content)}'
            })
            current_segment = []
            current_size = 0
        
        current_segment.append(line)
        current_size += line_size
    
    # æœ€å¾Œä¸€æ®µ
    if current_segment:
        segment_content = '\n'.join(current_segment)
        segments.append({
            'content': segment_content,
            'range': f'{len(segments) * max_chars_per_segment}-{len(segments) * max_chars_per_segment + len(segment_content)}'
        })
    
    return segments

def split_by_size_with_overlap(content, max_chars_per_segment):
    """æŒ‰å›ºå®šå¤§å°åˆ†æ®µï¼ˆå¸¶é‡ç–Šï¼‰"""
    segments = []
    overlap_size = max_chars_per_segment // 10  # 10% é‡ç–Š
    
    start = 0
    segment_num = 0
    
    while start < len(content):
        end = start + max_chars_per_segment
        
        if end >= len(content):
            # æœ€å¾Œä¸€æ®µ
            segment_content = content[start:]
        else:
            # å˜—è©¦åœ¨æ›è¡Œè™•æˆªæ–·
            segment_content = content[start:end]
            last_newline = segment_content.rfind('\n')
            if last_newline > max_chars_per_segment * 0.8:
                end = start + last_newline
                segment_content = content[start:end]
        
        segments.append({
            'content': segment_content,
            'range': f'{start}-{end}'
        })
        
        start = end - overlap_size
        segment_num += 1
        
        if start >= len(content):
            break
    
    return segments

def generate_comprehensive_analysis(segment_results, file_path, provider, mode):
    """ç”Ÿæˆç¶œåˆåˆ†æ"""
    successful_results = [s for s in segment_results if s.get('success') and s.get('analysis')]
    
    if not successful_results:
        return "ç„¡æ³•ç”Ÿæˆç¶œåˆåˆ†æï¼šæ‰€æœ‰æ®µè½åˆ†æéƒ½å¤±æ•—äº†ã€‚"
    
    # åˆä½µæ‰€æœ‰æˆåŠŸçš„åˆ†æçµæœ
    all_analyses = [s['analysis'] for s in successful_results]
    
    # å‰µå»ºç¶œåˆåˆ†ææç¤ºè©
    synthesis_prompt = f"""
åŸºæ–¼ä»¥ä¸‹ {len(all_analyses)} å€‹æ®µè½çš„åˆ†æçµæœï¼Œè«‹ç”Ÿæˆä¸€å€‹ç¶œåˆçš„åˆ†æå ±å‘Šï¼š

æª”æ¡ˆï¼š{file_path}
åˆ†ææ¨¡å¼ï¼š{mode.value}

æ®µè½åˆ†æçµæœï¼š
{chr(10).join([f"æ®µè½ {i+1}: {analysis[:500]}..." for i, analysis in enumerate(all_analyses)])}

è«‹æä¾›ï¼š
1. ç¶œåˆå•é¡Œæ‘˜è¦
2. ä¸»è¦ç™¼ç¾å’Œæ ¹æœ¬åŸå› 
3. å½±éŸ¿ç¯„åœè©•ä¼°
4. çµ±ä¸€çš„è§£æ±ºæ–¹æ¡ˆå»ºè­°
5. é é˜²æªæ–½

è«‹ç¢ºä¿åˆ†æçµæœé€£è²«ä¸”å®Œæ•´ï¼Œé¿å…é‡è¤‡å…§å®¹ã€‚
"""
    
    try:
        # ä½¿ç”¨ provider ç”Ÿæˆç¶œåˆåˆ†æ
        context = AnalysisContext(
            file_path=file_path,
            file_content=synthesis_prompt,
            mode=mode,
            previous_messages=[],
            metadata={'is_synthesis': True}
        )
        
        return provider.analyze_sync(context)
    except Exception as e:
        print(f"ç”Ÿæˆç¶œåˆåˆ†æå¤±æ•—: {str(e)}")
        # é™ç´šæ–¹æ¡ˆï¼šç°¡å–®åˆä½µ
        return f"""
# åˆ†æ®µåˆ†æç¶œåˆå ±å‘Š

## ğŸ“Š åˆ†ææ¦‚æ³
- ç¸½æ®µè½æ•¸ï¼š{len(segment_results)}
- æˆåŠŸåˆ†æï¼š{len(successful_results)} æ®µ
- å¤±æ•—æ®µè½ï¼š{len(segment_results) - len(successful_results)} æ®µ

## ğŸ” ä¸»è¦ç™¼ç¾

{chr(10).join([f"### æ®µè½ {s['segment_number']} åˆ†æçµæœ{chr(10)}{s['analysis'][:1000]}...{chr(10)}" for s in successful_results[:3]])}

## ğŸ’¡ ç¶œåˆå»ºè­°
åŸºæ–¼ä»¥ä¸Šåˆ†æ®µåˆ†æï¼Œå»ºè­°é€²ä¸€æ­¥æŸ¥çœ‹å„æ®µè½çš„è©³ç´°åˆ†æçµæœã€‚
"""