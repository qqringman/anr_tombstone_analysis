from flask import Blueprint, request, jsonify, Response, stream_with_context
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
import os
import time
import json
import threading
from typing import Dict, List, Optional, Any
import anthropic
from config.config import AI_PROVIDERS, ANALYSIS_MODES, TOKEN_PRICING, RATE_LIMITS, MODEL_LIMITS, CLAUDE_API_KEY
from enum import Enum
from collections import deque

ai_analyzer_bp = Blueprint('ai_analyzer_bp', __name__)

# å…¨åŸŸè®Šæ•¸ç”¨æ–¼ç®¡ç†é€²è¡Œä¸­çš„åˆ†æ
active_analyses: Dict[str, 'AnalysisSession'] = {}

class AnalysisMode(Enum):
    SMART = "smart"
    QUICK = "quick" 
    DEEP = "deep"

class MessageRole(Enum):
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"

@dataclass
class Message:
    role: MessageRole
    content: str
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

@dataclass
class AnalysisContext:
    file_path: str
    file_content: str
    mode: AnalysisMode
    previous_messages: List[Message]
    metadata: Dict[str, Any]

class AIProvider(ABC):
    """AI Provider çš„æŠ½è±¡åŸºé¡"""
    
    def __init__(self, api_key: str, model: str):
        self.api_key = api_key
        self.model = model
        self._stop_flag = threading.Event()
        
    @abstractmethod
    def analyze_sync(self, context: AnalysisContext) -> str:
        """åŒæ­¥åˆ†æ"""
        pass
    
    @abstractmethod
    def stream_analyze_sync(self, context: AnalysisContext):
        """åŒæ­¥æµå¼åˆ†æ"""
        pass
    
    def calculate_tokens(self, text: str) -> int:
        """è¨ˆç®— token æ•¸é‡"""
        return len(text) // 4  # ç°¡å–®ä¼°ç®—
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è¨ˆç®—æˆæœ¬"""
        return 0.0  # ç°¡åŒ–ç‰ˆæœ¬
    
    def stop(self):
        """åœæ­¢ç•¶å‰çš„åˆ†æ"""
        self._stop_flag.set()
    
    def reset_stop_flag(self):
        """é‡ç½®åœæ­¢æ¨™è¨˜"""
        self._stop_flag.clear()
    
    def should_stop(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²åœæ­¢"""
        return self._stop_flag.is_set()

class AnthropicProvider(AIProvider):
    """Anthropic Claude Provider"""
    
    def __init__(self, api_key: str, model: str):
        super().__init__(api_key, model)
        self.client = anthropic.Anthropic(api_key=api_key)
        
        # å¾é…ç½®ç²å–æ¨¡å‹é™åˆ¶
        self.model_config = MODEL_LIMITS.get(model, {})
        self.max_tokens = self.model_config.get('max_tokens', 200000)
        self.max_output_tokens = self.model_config.get('max_output_tokens', 4096)
        self.chars_per_token = self.model_config.get('chars_per_token', 4)

        # ç²å–é€Ÿç‡é™åˆ¶ç®¡ç†å™¨
        self.rate_limiter = get_rate_limiter('anthropic')

    def calculate_tokens(self, text: str) -> int:
        """è¨ˆç®— token æ•¸é‡"""
        if not text:
            return 0
        # ä½¿ç”¨æ¨¡å‹é…ç½®çš„ chars_per_token æˆ–é»˜èªå€¼ 4
        return len(text) // self.chars_per_token
    
    def get_cost(self, input_tokens: int, output_tokens: int) -> float:
        """è¨ˆç®—æˆæœ¬"""
        pricing = TOKEN_PRICING.get('anthropic', {}).get(self.model, {'input': 0, 'output': 0})
        input_cost = (input_tokens / 1000) * pricing.get('input', 0)
        output_cost = (output_tokens / 1000) * pricing.get('output', 0)
        return input_cost + output_cost
    
    def _truncate_content(self, content: str, mode: AnalysisMode) -> tuple[str, bool]:
        """æ ¹æ“š token é™åˆ¶æˆªå–å…§å®¹"""
        # é ç•™ç©ºé–“çµ¦ç³»çµ±æç¤ºå’Œä¸Šä¸‹æ–‡
        reserved_tokens = 5000  # é ç•™çµ¦ç³»çµ±æç¤ºã€æ­·å²è¨Šæ¯ç­‰
        
        # æ ¹æ“šæ¨¡å¼èª¿æ•´ç­–ç•¥
        if mode == AnalysisMode.QUICK:
            # å¿«é€Ÿåˆ†æï¼šåªå–å‰å¾Œé—œéµéƒ¨åˆ†
            max_content_tokens = min(50000, self.max_tokens - reserved_tokens)
        elif mode == AnalysisMode.DEEP:
            # æ·±åº¦åˆ†æï¼šç›¡å¯èƒ½å¤šçš„å…§å®¹
            max_content_tokens = self.max_tokens - reserved_tokens
        else:  # SMART
            # æ™ºèƒ½åˆ†æï¼šå¹³è¡¡çš„å…§å®¹é‡
            max_content_tokens = min(100000, self.max_tokens - reserved_tokens)
        
        # ä¼°ç®—ç•¶å‰å…§å®¹çš„ tokens
        current_tokens = self.calculate_tokens(content)
        
        if current_tokens <= max_content_tokens:
            return content, False
        
        # éœ€è¦æˆªå–
        max_chars = max_content_tokens * self.chars_per_token
        
        if mode == AnalysisMode.QUICK:
            # å¿«é€Ÿæ¨¡å¼ï¼šå–é–‹é ­å’Œçµå°¾
            head_chars = max_chars // 3
            tail_chars = max_chars // 3
            
            truncated = (
                content[:head_chars] + 
                f"\n\n... [çœç•¥ä¸­é–“å…§å®¹ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ] ...\n\n" +
                content[-tail_chars:]
            )
        else:
            # å…¶ä»–æ¨¡å¼ï¼šå„ªå…ˆä¿ç•™é–‹é ­ï¼ˆé€šå¸¸åŒ…å«é‡è¦ä¿¡æ¯ï¼‰
            truncated = content[:max_chars] + f"\n\n... [å…§å®¹å·²æˆªå–ï¼ŒåŸå§‹å¤§å°: {len(content)} å­—å…ƒ]"
        
        return truncated, True
    
    def analyze_sync(self, context: AnalysisContext) -> str:
        """åŒæ­¥åˆ†æ"""
        retry_count = 0
        max_retries = 3
        base_delay = 5  # åŸºç¤å»¶é²5ç§’
        
        while retry_count <= max_retries:
            try:
                messages = self._prepare_messages(context)
                response = self.client.messages.create(
                    model=self.model,
                    messages=messages,
                    max_tokens=self._get_max_tokens(context.mode),
                    temperature=self._get_temperature(context.mode)
                )
                return response.content[0].text
            except Exception as e:
                error_str = str(e)
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯éè¼‰éŒ¯èª¤
                if "overloaded" in error_str.lower():
                    retry_count += 1
                    if retry_count <= max_retries:
                        # æŒ‡æ•¸é€€é¿ç­–ç•¥
                        delay = base_delay * (2 ** (retry_count - 1))
                        print(f"API éè¼‰ï¼Œç­‰å¾… {delay} ç§’å¾Œé‡è©¦... (ç¬¬ {retry_count}/{max_retries} æ¬¡)")
                        time.sleep(delay)
                        continue
                    else:
                        raise Exception("API æŒçºŒéè¼‰ï¼Œè«‹ç¨å¾Œå†è©¦ï¼ˆå»ºè­°ç­‰å¾… 1-2 åˆ†é˜ï¼‰")
                
                # å…¶ä»–éŒ¯èª¤ç›´æ¥æ‹‹å‡º
                raise Exception(f"Anthropic API éŒ¯èª¤: {error_str}")
    
    def stream_analyze_sync(self, context: AnalysisContext):
        """åŒæ­¥æµå¼åˆ†æ"""
        self.reset_stop_flag()
        
        # ä¼°ç®— token ä½¿ç”¨é‡
        messages = self._prepare_messages(context)
        estimated_input_tokens = sum(self.calculate_tokens(str(msg)) for msg in messages)
        estimated_total_tokens = estimated_input_tokens + self.max_output_tokens
        
        # ç²å–ä½¿ç”¨çµ±è¨ˆ
        usage_stats = self.rate_limiter.get_usage_stats()
        
        # ç™¼é€é€Ÿç‡é™åˆ¶ä¿¡æ¯
        yield json.dumps({
            'type': 'rate_limit_info',
            'usage': usage_stats,
            'estimated_tokens': estimated_total_tokens
        }) + '\n'
        
        # æª¢æŸ¥é€Ÿç‡é™åˆ¶
        can_proceed, wait_time = self.rate_limiter.can_make_request(estimated_total_tokens)
        
        if not can_proceed:
            # å¦‚æœéœ€è¦ç­‰å¾…æ™‚é–“å°æ–¼30ç§’ï¼Œè‡ªå‹•ç­‰å¾…
            if wait_time <= 30:
                yield json.dumps({
                    'type': 'rate_limit_wait',
                    'wait_time': wait_time,
                    'reason': self._get_rate_limit_reason(usage_stats)
                }) + '\n'
                
                # ç­‰å¾…ä¸¦é‡è©¦
                if self._wait_for_rate_limit(estimated_total_tokens):
                    can_proceed = True
                else:
                    yield json.dumps({
                        'type': 'error',
                        'error': 'é€Ÿç‡é™åˆ¶ï¼šè«‹ç¨å¾Œå†è©¦',
                        'usage_stats': usage_stats
                    }) + '\n'
                    return
            else:
                # ç­‰å¾…æ™‚é–“å¤ªé•·ï¼Œç›´æ¥è¿”å›éŒ¯èª¤
                yield json.dumps({
                    'type': 'error',
                    'error': f'å·²é”åˆ°é€Ÿç‡é™åˆ¶ï¼Œè«‹ç­‰å¾… {int(wait_time)} ç§’å¾Œå†è©¦',
                    'usage_stats': usage_stats
                }) + '\n'
                return
        
        # è¨˜éŒ„è«‹æ±‚é–‹å§‹
        request_start_time = time.time()
        actual_output_tokens = 0
        
        try:
            # ä½¿ç”¨ stream
            with self.client.messages.stream(
                model=self.model,
                messages=messages,
                max_tokens=self._get_max_tokens(context.mode),
                temperature=self._get_temperature(context.mode)
            ) as stream:
                for text in stream.text_stream:
                    if self.should_stop():
                        break
                    actual_output_tokens += self.calculate_tokens(text)
                    yield json.dumps({
                        'type': 'content',
                        'content': text
                    }) + '\n'
                
                # è¨˜éŒ„å¯¦éš›ä½¿ç”¨çš„ tokens
                actual_total_tokens = estimated_input_tokens + actual_output_tokens
                self.rate_limiter.record_request(actual_total_tokens)
                
                # ç™¼é€å®Œæˆä¿¡æ¯ï¼ŒåŒ…å«é€Ÿç‡é™åˆ¶ç‹€æ…‹
                final_usage = self.rate_limiter.get_usage_stats()
                yield json.dumps({
                    'type': 'complete',
                    'usage': {
                        'input': estimated_input_tokens,
                        'output': actual_output_tokens,
                        'total': actual_total_tokens
                    },
                    'rate_limit_status': final_usage
                }) + '\n'
                    
        except Exception as e:
            error_str = str(e)
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶éŒ¯èª¤
            if any(keyword in error_str.lower() for keyword in ['rate', 'limit', 'overloaded', '429']):
                # è¨˜éŒ„å¤±æ•—çš„è«‹æ±‚ï¼ˆä½¿ç”¨ä¼°ç®—çš„ tokensï¼‰
                self.rate_limiter.record_request(estimated_total_tokens)
                
                current_usage = self.rate_limiter.get_usage_stats()
                yield json.dumps({
                    'type': 'error',
                    'error': 'é”åˆ° API é€Ÿç‡é™åˆ¶',
                    'error_detail': error_str,
                    'usage_stats': current_usage,
                    'retry_after': 60  # å»ºè­°ç­‰å¾…60ç§’
                }) + '\n'
            else:
                yield json.dumps({
                    'type': 'error',
                    'error': f'åˆ†æéŒ¯èª¤: {error_str}'
                }) + '\n'
    
    def _prepare_messages(self, context: AnalysisContext) -> List[Dict]:
        """æº–å‚™è¨Šæ¯æ ¼å¼"""
        messages = []
        
        # æˆªå–æª”æ¡ˆå…§å®¹
        truncated_content, was_truncated = self._truncate_content(
            context.file_content, 
            context.mode
        )
        
        # æ·»åŠ ç³»çµ±æç¤º
        system_prompt = self._get_system_prompt(context.mode)
        
        # å¦‚æœå…§å®¹è¢«æˆªå–ï¼Œåœ¨ç³»çµ±æç¤ºä¸­èªªæ˜
        if was_truncated:
            system_prompt += "\n\næ³¨æ„ï¼šç”±æ–¼æª”æ¡ˆéå¤§ï¼Œåªæä¾›äº†éƒ¨åˆ†å…§å®¹é€²è¡Œåˆ†æã€‚è«‹åŸºæ–¼å¯è¦‹çš„å…§å®¹æä¾›åˆ†æã€‚"
        
        # æ·»åŠ æ­·å²è¨Šæ¯ï¼ˆé™åˆ¶æ•¸é‡ï¼‰
        recent_messages = context.previous_messages[-3:]  # åªä¿ç•™æœ€è¿‘3æ¢
        for msg in recent_messages:
            if msg.role == MessageRole.USER:
                # é™åˆ¶æ­·å²è¨Šæ¯é•·åº¦
                content = msg.content[:1000] if len(msg.content) > 1000 else msg.content
                messages.append({"role": "user", "content": content})
            elif msg.role == MessageRole.ASSISTANT:
                content = msg.content[:2000] if len(msg.content) > 2000 else msg.content
                messages.append({"role": "assistant", "content": content})
        
        # æ·»åŠ ç•¶å‰è«‹æ±‚
        user_message = f"{system_prompt}\n\næª”æ¡ˆè·¯å¾‘: {context.file_path}\n\næª”æ¡ˆå…§å®¹:\n{truncated_content}"
        messages.append({"role": "user", "content": user_message})
        
        # å°‡æˆªå–ç‹€æ…‹ä¿å­˜åˆ°ä¸Šä¸‹æ–‡
        context.metadata['was_truncated'] = was_truncated
        context.metadata['original_size'] = len(context.file_content)
        context.metadata['truncated_size'] = len(truncated_content)
        context.metadata['actual_input_tokens'] = self.calculate_tokens(user_message)
        
        return messages

    def _wait_for_rate_limit(self, estimated_tokens: int) -> bool:
        """ç­‰å¾…é€Ÿç‡é™åˆ¶"""
        max_wait = 60  # æœ€å¤šç­‰å¾…60ç§’
        start_time = time.time()
        
        while time.time() - start_time < max_wait:
            can_proceed, wait_time = self.rate_limiter.can_make_request(estimated_tokens)
            
            if can_proceed:
                return True
            
            if wait_time > max_wait - (time.time() - start_time):
                # ç­‰å¾…æ™‚é–“å¤ªé•·
                return False
            
            # æ™ºèƒ½ç­‰å¾…
            actual_wait = min(wait_time + 1, 5)  # æœ€å¤šç­‰5ç§’å¾Œé‡æ–°æª¢æŸ¥
            print(f"é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {actual_wait:.1f} ç§’...")
            time.sleep(actual_wait)
        
        return False

    def _get_rate_limit_reason(self, usage_stats: dict) -> str:
        """ç²å–é€Ÿç‡é™åˆ¶åŸå› """
        reasons = []
        
        if usage_stats['rpm_current'] >= usage_stats['rpm_limit']:
            reasons.append(f"æ¯åˆ†é˜è«‹æ±‚æ•¸å·²é”ä¸Šé™ ({usage_stats['rpm_current']}/{usage_stats['rpm_limit']})")
        
        if usage_stats['tpm_current'] >= usage_stats['tpm_limit']:
            reasons.append(f"æ¯åˆ†é˜ Token æ•¸å·²é”ä¸Šé™ ({usage_stats['tpm_current']:,}/{usage_stats['tpm_limit']:,})")
        
        if usage_stats['tpd_current'] >= usage_stats['tpd_limit'] * 0.9:  # æ¥è¿‘æ¯æ—¥é™åˆ¶
            reasons.append(f"æ¥è¿‘æ¯æ—¥ Token é™åˆ¶ ({usage_stats['tpd_current']:,}/{usage_stats['tpd_limit']:,})")
        
        return " | ".join(reasons) if reasons else "API ç¹å¿™"

    def _get_system_prompt(self, mode: AnalysisMode) -> str:
        """æ ¹æ“šæ¨¡å¼ç²å–ç³»çµ±æç¤º"""
        prompts = {
            AnalysisMode.SMART: """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ Android ç³»çµ±æ—¥èªŒåˆ†æå°ˆå®¶ã€‚è«‹åˆ†ææä¾›çš„æ—¥èªŒå…§å®¹ï¼š

1. è‡ªå‹•è­˜åˆ¥æ—¥èªŒé¡å‹ï¼š
   - å¦‚æœåŒ…å« "Subject: Input dispatching timed out" æˆ–é¡ä¼¼å…§å®¹ï¼Œé€™æ˜¯ ANR (Application Not Responding) æ—¥èªŒ
   - å¦‚æœåŒ…å« "tombstone" æˆ–å´©æ½°å †ç–Šï¼Œé€™æ˜¯å´©æ½°æ—¥èªŒ
   - å…¶ä»–é¡å‹è«‹ç›¸æ‡‰è­˜åˆ¥

2. æ ¹æ“šæ—¥èªŒé¡å‹æä¾›åˆ†æï¼š
   - **å•é¡Œæ‘˜è¦**ï¼šç°¡æ½”æè¿°ç™¼ç”Ÿäº†ä»€éº¼å•é¡Œ
   - **æ ¹æœ¬åŸå› **ï¼šæ·±å…¥åˆ†æå°è‡´å•é¡Œçš„åŸå› 
   - **å½±éŸ¿ç¯„åœ**ï¼šå“ªäº›é€²ç¨‹/çµ„ä»¶å—åˆ°å½±éŸ¿
   - **è§£æ±ºæ–¹æ¡ˆ**ï¼šæä¾›å…·é«”çš„ä¿®å¾©å»ºè­°
   
3. è«‹ä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œå’Œæ ¼å¼åŒ–è¼¸å‡º""",
            
            AnalysisMode.QUICK: """ä½ æ˜¯ä¸€å€‹å¿«é€Ÿè¨ºæ–·åŠ©æ‰‹ã€‚åœ¨ 30 ç§’å…§åˆ†æé€™å€‹ Android æ—¥èªŒï¼š

å¿«é€Ÿè­˜åˆ¥ï¼š
- æ—¥èªŒé¡å‹ï¼ˆANR/å´©æ½°/å…¶ä»–ï¼‰
- ä¸»è¦å•é¡Œ
- ç«‹å³å¯åŸ·è¡Œçš„è§£æ±ºæ–¹æ¡ˆ

è«‹ä¿æŒç°¡æ½”ï¼Œåªæä¾›æœ€é—œéµçš„ä¿¡æ¯ã€‚""",
            
            AnalysisMode.DEEP: """ä½ æ˜¯ä¸€å€‹ç´°ç·»çš„ Android ç³»çµ±åˆ†æå°ˆå®¶ã€‚è«‹å°é€™å€‹æ—¥èªŒé€²è¡Œæ·±åº¦åˆ†æï¼š

1. **æ—¥èªŒè§£æ**
   - å®Œæ•´è§£è®€æ¯å€‹é—œéµä¿¡æ¯
   - åˆ†æè¨˜æ†¶é«”åœ°å€ã€æš«å­˜å™¨ç‹€æ…‹
   - è¿½è¹¤å®Œæ•´çš„èª¿ç”¨å †ç–Š

2. **å•é¡Œè¨ºæ–·**
   - è©³ç´°çš„æ ¹å› åˆ†æ
   - å¯èƒ½çš„è§¸ç™¼æ¢ä»¶
   - ç³»çµ±ç‹€æ…‹è©•ä¼°

3. **è§£æ±ºæ–¹æ¡ˆ**
   - ç¨‹å¼ç¢¼å±¤é¢çš„ä¿®å¾©å»ºè­°
   - ç³»çµ±é…ç½®å„ªåŒ–
   - é é˜²æªæ–½

4. **ç›¸é—œçŸ¥è­˜**
   - ç›¸é—œçš„ Android ç³»çµ±çŸ¥è­˜
   - é¡ä¼¼å•é¡Œçš„è™•ç†ç¶“é©—"""
        }
        return prompts.get(mode, prompts[AnalysisMode.SMART])
    
    def _get_max_tokens(self, mode: AnalysisMode) -> int:
        """æ ¹æ“šæ¨¡å¼ç²å–æœ€å¤§è¼¸å‡º token æ•¸"""
        # ä½¿ç”¨æ¨¡å‹é…ç½®çš„æœ€å¤§è¼¸å‡º tokensï¼Œä½†æ ¹æ“šæ¨¡å¼èª¿æ•´
        base_max = self.max_output_tokens
        
        tokens = {
            AnalysisMode.QUICK: min(1000, base_max),
            AnalysisMode.SMART: min(2000, base_max),
            AnalysisMode.DEEP: min(4000, base_max)
        }
        return tokens.get(mode, min(2000, base_max))
    
    def _get_temperature(self, mode: AnalysisMode) -> float:
        """æ ¹æ“šæ¨¡å¼ç²å–æº«åº¦åƒæ•¸"""
        temps = {
            AnalysisMode.QUICK: 0.3,
            AnalysisMode.SMART: 0.5,
            AnalysisMode.DEEP: 0.7
        }
        return temps.get(mode, 0.5)

class ProviderFactory:
    """AI Provider å·¥å» é¡"""
    
    @staticmethod
    def create_provider(provider_name: str, model: str) -> AIProvider:
        """å‰µå»º AI Provider å¯¦ä¾‹"""
        if provider_name not in AI_PROVIDERS:
            raise ValueError(f"ä¸æ”¯æ´çš„ Provider: {provider_name}")
        
        config = AI_PROVIDERS[provider_name]
        api_key = config['api_key']
        
        if not api_key:
            raise ValueError(f"{provider_name} API Key æœªè¨­ç½®")
        
        if provider_name == 'anthropic':
            return AnthropicProvider(api_key, model)
        else:
            raise ValueError(f"æœªå¯¦ä½œçš„ Provider: {provider_name}")

class AnalysisSession:
    """åˆ†ææœƒè©±ç®¡ç†"""
    
    def __init__(self, session_id: str, provider: AIProvider):
        self.session_id = session_id
        self.provider = provider
        self.messages: List[Message] = []
        self.created_at = datetime.now()
        self.last_activity = datetime.now()
        self.is_active = True
        self.total_cost = 0.0
        self.total_tokens = {'input': 0, 'output': 0}
        
    def add_message(self, role: MessageRole, content: str):
        """æ·»åŠ è¨Šæ¯åˆ°æœƒè©±"""
        self.messages.append(Message(role, content))
        self.last_activity = datetime.now()
        
    def stop(self):
        """åœæ­¢ç•¶å‰åˆ†æ"""
        self.provider.stop()
        self.is_active = False
        
    def update_usage(self, input_tokens: int, output_tokens: int):
        """æ›´æ–°ä½¿ç”¨é‡çµ±è¨ˆ"""
        self.total_tokens['input'] += input_tokens
        self.total_tokens['output'] += output_tokens
        self.total_cost += self.provider.get_cost(input_tokens, output_tokens)

class RateLimitManager:
    """é€Ÿç‡é™åˆ¶ç®¡ç†å™¨"""
    
    def __init__(self, provider: str = 'anthropic', tier: str = 'tier1'):
        self.provider = provider
        self.tier = tier
        self.limits = RATE_LIMITS.get(provider, {}).get(tier, {})
        
        # è«‹æ±‚æ­·å²è¨˜éŒ„
        self.request_history = deque()  # (timestamp, tokens)
        self.lock = threading.Lock()
        
        # ç•¶å‰ä½¿ç”¨çµ±è¨ˆ
        self.minute_requests = 0
        self.minute_tokens = 0
        self.day_tokens = 0
        self.last_reset_minute = time.time()
        self.last_reset_day = time.time()
        
    def can_make_request(self, estimated_tokens: int) -> tuple[bool, float]:
        """
        æª¢æŸ¥æ˜¯å¦å¯ä»¥ç™¼é€è«‹æ±‚
        è¿”å›: (æ˜¯å¦å¯ä»¥, éœ€è¦ç­‰å¾…çš„ç§’æ•¸)
        """
        with self.lock:
            current_time = time.time()
            
            # æ¸…ç†éæœŸçš„è«‹æ±‚è¨˜éŒ„
            self._cleanup_history(current_time)
            
            # æª¢æŸ¥æ¯åˆ†é˜è«‹æ±‚æ•¸ (RPM)
            rpm_limit = self.limits.get('rpm', 50)
            current_rpm = self._count_requests_in_window(60)
            if current_rpm >= rpm_limit:
                wait_time = 60 - (current_time - self.request_history[0][0])
                return False, wait_time
            
            # æª¢æŸ¥æ¯åˆ†é˜ token æ•¸ (TPM)
            tpm_limit = self.limits.get('tpm', 50000)
            current_tpm = self._count_tokens_in_window(60)
            if current_tpm + estimated_tokens > tpm_limit:
                wait_time = 60 - (current_time - self.request_history[0][0])
                return False, wait_time
            
            # æª¢æŸ¥æ¯æ—¥ token æ•¸ (TPD)
            tpd_limit = self.limits.get('tpd', 1000000)
            current_tpd = self._count_tokens_in_window(86400)  # 24å°æ™‚
            if current_tpd + estimated_tokens > tpd_limit:
                # å»ºè­°ç­‰å¾…åˆ°ç¬¬äºŒå¤©
                return False, 3600  # è¿”å›1å°æ™‚ä½œç‚ºå»ºè­°ç­‰å¾…æ™‚é–“
            
            return True, 0
    
    def record_request(self, tokens_used: int):
        """è¨˜éŒ„ä¸€æ¬¡è«‹æ±‚"""
        with self.lock:
            self.request_history.append((time.time(), tokens_used))
    
    def _cleanup_history(self, current_time: float):
        """æ¸…ç†è¶…é24å°æ™‚çš„è¨˜éŒ„"""
        cutoff_time = current_time - 86400  # 24å°æ™‚å‰
        while self.request_history and self.request_history[0][0] < cutoff_time:
            self.request_history.popleft()
    
    def _count_requests_in_window(self, window_seconds: int) -> int:
        """è¨ˆç®—æ™‚é–“çª—å£å…§çš„è«‹æ±‚æ•¸"""
        cutoff_time = time.time() - window_seconds
        count = 0
        for timestamp, _ in self.request_history:
            if timestamp >= cutoff_time:
                count += 1
        return count
    
    def _count_tokens_in_window(self, window_seconds: int) -> int:
        """è¨ˆç®—æ™‚é–“çª—å£å…§çš„ token ç¸½æ•¸"""
        cutoff_time = time.time() - window_seconds
        total_tokens = 0
        for timestamp, tokens in self.request_history:
            if timestamp >= cutoff_time:
                total_tokens += tokens
        return total_tokens
    
    def get_usage_stats(self) -> dict:
        """ç²å–ç•¶å‰ä½¿ç”¨çµ±è¨ˆ"""
        with self.lock:
            current_time = time.time()
            self._cleanup_history(current_time)
            
            return {
                'rpm_current': self._count_requests_in_window(60),
                'rpm_limit': self.limits.get('rpm', 50),
                'tpm_current': self._count_tokens_in_window(60),
                'tpm_limit': self.limits.get('tpm', 50000),
                'tpd_current': self._count_tokens_in_window(86400),
                'tpd_limit': self.limits.get('tpd', 1000000),
            }

# å…¨å±€é€Ÿç‡é™åˆ¶ç®¡ç†å™¨å¯¦ä¾‹
rate_limiters = {}

def get_rate_limiter(provider: str, tier: str = None) -> RateLimitManager:
    """ç²å–æˆ–å‰µå»ºé€Ÿç‡é™åˆ¶ç®¡ç†å™¨"""
    if tier is None:
        # æ ¹æ“š API key æˆ–å…¶ä»–é‚è¼¯åˆ¤æ–· tier
        # é€™è£¡é è¨­ä½¿ç”¨ tier2
        tier = 'tier2'
    
    key = f"{provider}_{tier}"
    if key not in rate_limiters:
        rate_limiters[key] = RateLimitManager(provider, tier)
    return rate_limiters[key]

# API è·¯ç”±
@ai_analyzer_bp.route('/api/ai/analyze', methods=['POST'])
def analyze_file():
    """åŸ·è¡Œ AI åˆ†æ"""
    try:
        data = request.json
        session_id = data.get('session_id', str(time.time()))
        provider_name = data.get('provider', 'anthropic')
        model = data.get('model', AI_PROVIDERS[provider_name]['default_model'])
        mode = AnalysisMode(data.get('mode', 'smart'))
        file_path = data.get('file_path', '')
        file_name = data.get('file_name', '')
        file_content = data.get('content', '')
        stream = data.get('stream', True)
        context_messages = data.get('context', [])
        
        # å‰µå»ºæˆ–ç²å–æœƒè©±
        if session_id in active_analyses:
            session = active_analyses[session_id]
        else:
            provider = ProviderFactory.create_provider(provider_name, model)
            session = AnalysisSession(session_id, provider)
            active_analyses[session_id] = session
        
        # æ·»åŠ ä¸Šä¸‹æ–‡è¨Šæ¯åˆ°æœƒè©±
        for msg in context_messages:
            if 'role' in msg and 'content' in msg:
                try:
                    role = MessageRole(msg['role']) if msg['role'] in ['user', 'assistant', 'system'] else MessageRole.USER
                    session.add_message(role, msg['content'])
                except ValueError:
                    # å¦‚æœè§’è‰²ç„¡æ•ˆï¼Œé»˜èªç‚ºç”¨æˆ¶
                    session.add_message(MessageRole.USER, msg['content'])
        
        # æº–å‚™åˆ†æä¸Šä¸‹æ–‡
        context = AnalysisContext(
            file_path=file_path,
            file_content=file_content,
            mode=mode,
            previous_messages=session.messages,
            metadata={
                'session_id': session_id,
                'file_name': file_name
            }
        )
        
        if stream:
            def generate():
                """ç”Ÿæˆ SSE æµ"""
                try:
                    # ç™¼é€é–‹å§‹äº‹ä»¶
                    yield f"data: {json.dumps({'type': 'start', 'mode': mode.value})}\n\n"
                    
                    # æª¢æŸ¥æª”æ¡ˆå¤§å°
                    file_size = len(file_content)
                    estimated_tokens = session.provider.calculate_tokens(file_content)
                    
                    # é¡¯ç¤ºæª”æ¡ˆè³‡è¨Š
                    file_info = f"æ­£åœ¨åˆ†ææª”æ¡ˆ: {file_path}"
                    if file_name:
                        file_info = f"æ­£åœ¨åˆ†ææª”æ¡ˆ: {file_name}"
                    
                    # è­˜åˆ¥æª”æ¡ˆé¡å‹
                    if 'tombstone' in file_path.lower() or 'tombstone' in file_content[:1000].lower():
                        file_info += " (Tombstone å´©æ½°æ—¥èªŒ)"
                    elif 'anr' in file_path.lower() or 'input dispatching timed out' in file_content[:5000].lower():
                        file_info += " (ANR æ—¥èªŒ)"
                    elif 'fatal' in file_content[:1000].lower() or 'crash' in file_content[:1000].lower():
                        file_info += " (å´©æ½°æ—¥èªŒ)"
                    
                    yield f"data: {json.dumps({'type': 'info', 'message': file_info})}\n\n"
                    
                    # é¡¯ç¤ºæª”æ¡ˆå¤§å°è³‡è¨Š
                    size_info = f"æª”æ¡ˆå¤§å°: {file_size:,} å­—å…ƒ (ç´„ {estimated_tokens:,} tokens)"
                    yield f"data: {json.dumps({'type': 'info', 'message': size_info})}\n\n"
                    
                    # å¦‚æœæª”æ¡ˆå¾ˆå¤§ï¼Œç™¼é€è­¦å‘Š
                    model_config = MODEL_LIMITS.get(model, {})
                    max_tokens = model_config.get('max_tokens', 200000)
                    
                    if estimated_tokens > max_tokens * 0.8:  # è¶…é 80% é™åˆ¶
                        warning_msg = f"âš ï¸ æª”æ¡ˆè¼ƒå¤§ï¼ˆç´„ {estimated_tokens:,} tokensï¼‰ï¼Œå°‡è‡ªå‹•æˆªå–é©ç•¶å…§å®¹é€²è¡Œåˆ†æ"
                        yield f"data: {json.dumps({'type': 'warning', 'message': warning_msg})}\n\n"
                    
                    # æ ¹æ“šæ¨¡å¼é¡¯ç¤ºé æœŸæ™‚é–“
                    if mode == AnalysisMode.QUICK:
                        yield f"data: {json.dumps({'type': 'info', 'message': 'âš¡ å¿«é€Ÿåˆ†ææ¨¡å¼ï¼šé è¨ˆ 30 ç§’å…§å®Œæˆ'})}\n\n"
                    elif mode == AnalysisMode.DEEP:
                        yield f"data: {json.dumps({'type': 'info', 'message': 'ğŸ” æ·±åº¦åˆ†ææ¨¡å¼ï¼šé è¨ˆ 2-3 åˆ†é˜å®Œæˆ'})}\n\n"
                    else:
                        yield f"data: {json.dumps({'type': 'info', 'message': 'ğŸ§  æ™ºèƒ½åˆ†ææ¨¡å¼ï¼šè‡ªå‹•å¹³è¡¡é€Ÿåº¦èˆ‡æ·±åº¦'})}\n\n"
                    
                    # ä¼°ç®—è¼¸å…¥ tokensï¼ˆå¯èƒ½æœƒè¢«æˆªå–ï¼‰
                    yield f"data: {json.dumps({'type': 'tokens', 'input': estimated_tokens})}\n\n"
                    
                    # æµå¼è¼¸å‡º
                    output_content = ""
                    chunk_count = 0
                    last_update_time = time.time()
                    
                    try:
                        for chunk in session.provider.stream_analyze_sync(context):
                            # æª¢æŸ¥æ˜¯å¦è¢«ä¸­æ–·
                            if not session.is_active:
                                yield f"data: {json.dumps({'type': 'stopped', 'message': 'åˆ†æå·²è¢«ä½¿ç”¨è€…ä¸­æ–·'})}\n\n"
                                break
                            
                            output_content += chunk
                            chunk_count += 1
                            
                            # ç™¼é€å…§å®¹
                            yield f"data: {json.dumps({'type': 'content', 'content': chunk})}\n\n"
                            
                            # å®šæœŸæ›´æ–° token è¨ˆæ•¸ï¼ˆæ¯ç§’æœ€å¤šä¸€æ¬¡ï¼‰
                            current_time = time.time()
                            if current_time - last_update_time > 1.0:
                                output_tokens = session.provider.calculate_tokens(output_content)
                                yield f"data: {json.dumps({'type': 'tokens', 'output': output_tokens})}\n\n"
                                last_update_time = current_time
                    
                    except Exception as stream_error:
                        # è™•ç†æµå¼è¼¸å‡ºä¸­çš„éŒ¯èª¤
                        error_msg = str(stream_error)
                        if "rate limit" in error_msg.lower():
                            error_msg = "è¶…é API é€Ÿç‡é™åˆ¶ï¼Œè«‹ç¨å¾Œå†è©¦ï¼ˆå»ºè­°ç­‰å¾… 30 ç§’ï¼‰"
                        elif "prompt is too long" in error_msg.lower():
                            error_msg = "æª”æ¡ˆå…§å®¹éå¤§ï¼Œè«‹å˜—è©¦ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼æˆ–åˆ†æ®µåˆ†æ"
                        
                        yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
                        return
                    
                    # å¦‚æœå…§å®¹è¢«æˆªå–ï¼Œåœ¨å®Œæˆæ™‚é€šçŸ¥
                    if context.metadata.get('was_truncated', False):
                        original_size = context.metadata.get('original_size', 0)
                        truncated_size = context.metadata.get('truncated_size', 0)
                        truncate_percentage = (truncated_size / original_size * 100) if original_size > 0 else 0
                        
                        truncate_info = {
                            'type': 'info',
                            'message': f"âœ‚ï¸ åˆ†æå®Œæˆã€‚ç”±æ–¼æª”æ¡ˆéå¤§ï¼Œå¯¦éš›åˆ†æäº† {truncated_size:,} / {original_size:,} å­—å…ƒ ({truncate_percentage:.1f}%)"
                        }
                        yield f"data: {json.dumps(truncate_info)}\n\n"
                    
                    # å®Œæˆåˆ†æ
                    if session.is_active and output_content:
                        # æ·»åŠ è¨Šæ¯åˆ°æœƒè©±æ­·å²
                        session.add_message(MessageRole.USER, f"åˆ†æ {mode.value} æ¨¡å¼: {file_name or file_path}")
                        session.add_message(MessageRole.ASSISTANT, output_content)
                        
                        # è¨ˆç®—æœ€çµ‚çš„ token ä½¿ç”¨é‡
                        final_output_tokens = session.provider.calculate_tokens(output_content)
                        actual_input_tokens = context.metadata.get('actual_input_tokens', estimated_tokens)
                        session.update_usage(actual_input_tokens, final_output_tokens)
                        
                        # ç™¼é€å®Œæˆäº‹ä»¶
                        yield f"data: {json.dumps({
                            'type': 'complete',
                            'usage': {
                                'input': session.total_tokens['input'],
                                'output': session.total_tokens['output'],
                                'total': session.total_tokens['input'] + session.total_tokens['output']
                            },
                            'cost': session.total_cost,
                            'was_truncated': context.metadata.get('was_truncated', False),
                            'duration': (datetime.now() - session.created_at).total_seconds()
                        })}\n\n"
                    
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    
                    error_msg = str(e)
                    # æä¾›æ›´å‹å¥½çš„éŒ¯èª¤è¨Šæ¯
                    if "prompt is too long" in error_msg:
                        error_msg = "æª”æ¡ˆå…§å®¹è¶…éæ¨¡å‹é™åˆ¶ï¼Œè«‹ä½¿ç”¨å¿«é€Ÿåˆ†ææ¨¡å¼æˆ–æ¸›å°‘æª”æ¡ˆå¤§å°"
                    elif "rate limit" in error_msg.lower():
                        error_msg = "API è«‹æ±‚éæ–¼é »ç¹ï¼Œè«‹ç­‰å¾… 30 ç§’å¾Œå†è©¦"
                    elif "api key" in error_msg.lower():
                        error_msg = "API Key ç„¡æ•ˆæˆ–æœªè¨­ç½®ï¼Œè«‹æª¢æŸ¥é…ç½®"
                    
                    yield f"data: {json.dumps({'type': 'error', 'error': error_msg})}\n\n"
            
            return Response(
                stream_with_context(generate()),
                mimetype='text/event-stream',
                headers={
                    'Cache-Control': 'no-cache',
                    'X-Accel-Buffering': 'no',
                    'Connection': 'keep-alive',
                    'Content-Type': 'text/event-stream'
                }
            )
        else:
            # éæµå¼å›æ‡‰
            try:
                result = session.provider.analyze_sync(context)
                
                # æª¢æŸ¥æ˜¯å¦è¢«æˆªå–
                was_truncated = context.metadata.get('was_truncated', False)
                
                # æ›´æ–°æœƒè©±
                session.add_message(MessageRole.USER, f"åˆ†ææª”æ¡ˆ: {file_name or file_path}")
                session.add_message(MessageRole.ASSISTANT, result)
                
                # è¨ˆç®— token ä½¿ç”¨é‡
                input_tokens = session.provider.calculate_tokens(file_content)
                output_tokens = session.provider.calculate_tokens(result)
                session.update_usage(input_tokens, output_tokens)
                
                response_data = {
                    'success': True,
                    'result': result,
                    'session_id': session_id,
                    'usage': {
                        'input': session.total_tokens['input'],
                        'output': session.total_tokens['output'],
                        'total': session.total_tokens['input'] + session.total_tokens['output']
                    },
                    'cost': session.total_cost,
                    'was_truncated': was_truncated
                }
                
                if was_truncated:
                    response_data['truncation_info'] = {
                        'original_size': context.metadata.get('original_size', 0),
                        'truncated_size': context.metadata.get('truncated_size', 0)
                    }
                
                return jsonify(response_data)
                
            except Exception as e:
                import traceback
                traceback.print_exc()
                
                error_msg = str(e)
                if "prompt is too long" in error_msg:
                    error_msg = "æª”æ¡ˆå…§å®¹è¶…éæ¨¡å‹é™åˆ¶"
                elif "rate limit" in error_msg.lower():
                    error_msg = "API è«‹æ±‚é™åˆ¶"
                
                return jsonify({
                    'success': False,
                    'error': error_msg,
                    'details': str(e)
                }), 500
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        
        return jsonify({
            'success': False,
            'error': 'è™•ç†è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤',
            'details': str(e)
        }), 500

@ai_analyzer_bp.route('/api/ai/stop/<session_id>', methods=['POST'])
def stop_analysis(session_id):
    """åœæ­¢åˆ†æ"""
    if session_id in active_analyses:
        session = active_analyses[session_id]
        session.stop()
        return jsonify({'success': True, 'message': 'åˆ†æå·²åœæ­¢'})
    return jsonify({'success': False, 'error': 'æœƒè©±ä¸å­˜åœ¨'}), 404

@ai_analyzer_bp.route('/api/ai/models/<provider>', methods=['GET'])
def get_models(provider):
    """ç²å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    if provider not in AI_PROVIDERS:
        return jsonify({'error': 'ä¸æ”¯æ´çš„ Provider'}), 400
        
    models = AI_PROVIDERS[provider]['models']
    return jsonify({
        'models': [
            {
                'id': model_id,
                'name': info['name'],
                'description': info['description'],
                'max_tokens': info['max_tokens'],
                'pricing': TOKEN_PRICING.get(provider, {}).get(model_id, {})
            }
            for model_id, info in models.items()
        ]
    })

@ai_analyzer_bp.route('/api/ai/rate-limit-status', methods=['GET'])
def get_rate_limit_status():
    """ç²å–ç•¶å‰é€Ÿç‡é™åˆ¶ç‹€æ…‹"""
    provider = request.args.get('provider', 'anthropic')
    
    try:
        limiter = get_rate_limiter(provider)
        stats = limiter.get_usage_stats()
        
        # è¨ˆç®—å‰©é¤˜é…é¡ç™¾åˆ†æ¯”
        rpm_percentage = (stats['rpm_current'] / stats['rpm_limit'] * 100) if stats['rpm_limit'] > 0 else 0
        tpm_percentage = (stats['tpm_current'] / stats['tpm_limit'] * 100) if stats['tpm_limit'] > 0 else 0
        tpd_percentage = (stats['tpd_current'] / stats['tpd_limit'] * 100) if stats['tpd_limit'] > 0 else 0
        
        return jsonify({
            'success': True,
            'provider': provider,
            'usage': {
                'requests_per_minute': {
                    'current': stats['rpm_current'],
                    'limit': stats['rpm_limit'],
                    'percentage': round(rpm_percentage, 1)
                },
                'tokens_per_minute': {
                    'current': stats['tpm_current'],
                    'limit': stats['tpm_limit'],
                    'percentage': round(tpm_percentage, 1)
                },
                'tokens_per_day': {
                    'current': stats['tpd_current'],
                    'limit': stats['tpd_limit'],
                    'percentage': round(tpd_percentage, 1)
                }
            },
            'status': 'normal' if max(rpm_percentage, tpm_percentage) < 80 else 'warning'
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

